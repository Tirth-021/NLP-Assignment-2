{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install --upgrade --force-reinstall google-api-python-client\n",
    "!pip install --upgrade tensorboard\n",
    "!pip install accelerate\n",
    "!pip install google-api-python-client\n",
    "!pip install numpy\n",
    "!pip install langid\n",
    "!pip install peft openpyxl bitsandbytes datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"GoogleCloudAPI\" \n",
    "my_token = \"HuggingFaceToken\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import json\n",
    "import os\n",
    "import gc\n",
    "\n",
    "print(\"--- 1. Loading Configuration ---\")\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B\"\n",
    "analysis_file = \"neuron_analysis_results_4WAY.json\"\n",
    "new_model_path = \"./edited_model_TOXIC_ABLATED\"\n",
    "target_neurons_key = \"english_toxic_neurons\"\n",
    "\n",
    "print(f\"--- 2. Loading original model: {model_id} ---\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=my_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    token=my_token\n",
    ")\n",
    "model.eval()\n",
    "print(\"Original model loaded successfully.\")\n",
    "\n",
    "print(f\"--- 3. Loading analysis results from {analysis_file} ---\")\n",
    "try:\n",
    "    with open(analysis_file, 'r') as f:\n",
    "        analysis_results = json.load(f)\n",
    "    neurons_to_edit = analysis_results[target_neurons_key]\n",
    "    print(f\"Successfully loaded neuron list for: {target_neurons_key}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Could not load {analysis_file}. Stopping. {e}\")\n",
    "    exit()\n",
    "\n",
    "print(f\"--- 4. Starting model surgery (ablating {target_neurons_key}) ---\")\n",
    "print(\"Zeroing out the 'down_proj' weights for target neurons...\")\n",
    "total_neurons_edited = 0\n",
    "total_layers_edited = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, layer in enumerate(model.model.layers):\n",
    "        layer_name = f\"model.model.layers.{i}.mlp\"\n",
    "        if layer_name in neurons_to_edit:\n",
    "            neuron_indices = neurons_to_edit[layer_name][\"top_neuron_indices\"]\n",
    "            target_weight_matrix = layer.mlp.down_proj.weight\n",
    "            target_weight_matrix[neuron_indices, :] = 0.0\n",
    "            if layer.mlp.down_proj.bias is not None:\n",
    "                layer.mlp.down_proj.bias[neuron_indices] = 0.0\n",
    "            total_neurons_edited += len(neuron_indices)\n",
    "            total_layers_edited += 1\n",
    "\n",
    "print(f\"--- 5. Surgery Complete ---\")\n",
    "print(f\"  Ablated {total_neurons_edited} neurons across {total_layers_edited} layers.\")\n",
    "\n",
    "print(f\"--- 6. Saving new, ablated model to {new_model_path} ---\")\n",
    "os.makedirs(new_model_path, exist_ok=True)\n",
    "model.save_pretrained(new_model_path)\n",
    "tokenizer.save_pretrained(new_model_path)\n",
    "\n",
    "print(f\"\\n--- Ablation Complete ---\")\n",
    "print(f\"'ablated' model is saved in '{new_model_path}'.\")\n",
    "\n",
    "del model\n",
    "del tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing cache and collecting garbage...\n",
      "VRAM cache cleared.\n",
      "--- 1. Loading Configuration ---\n",
      "--- 2. Loading and combining BENIGN-ONLY data (EN, HI, MR) ---\n",
      "  Loaded 1342 BENIGN samples from ./english_2021.csv.\n",
      "  Loaded 3161 BENIGN samples from ./hindi_2021.csv.\n",
      "  Loaded 1205 BENIGN samples from ./marathi_2021.csv.\n",
      "  Loaded 3591 BENIGN samples from ./english_2019_1.tsv.\n",
      "  Loaded 865 BENIGN samples from ./english_2019_2.tsv.\n",
      "  Loaded 713 BENIGN samples from ./hindi_2019_1.tsv.\n",
      "  Loaded 2196 BENIGN samples from ./hindi_2019_2.tsv.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loaded 1852 BENIGN samples from ./english_2020.xlsx.\n",
      "  Loaded 2116 BENIGN samples from ./hindi_2020.xlsx.\n",
      "\n",
      "Total combined multilingual BENIGN dataset size: 17041 samples.\n",
      "--- 3. Loading ABLATED model in 4-bit (QLoRA): ./edited_model_TOXIC_ABLATED ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0740f2b35b3465290eab4e061c590b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ablated model loaded in 4-bit.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a245eda969a146bd92db286719ef963a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/17041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset tokenized and ready.\n",
      "--- 5. Configuring QLoRA ---\n",
      "QLoRA adapters applied to model.\n",
      "trainable params: 41,943,040 || all params: 8,072,204,288 || trainable%: 0.5196\n",
      "--- 6. Configuring Trainer ---\n",
      "--- 7. Starting 'Healing' Finetuning on BENIGN-ONLY data ---\n",
      "Checkpoints will be saved to 'llama-3.1-8b-ablated-healed-BENIGN-ONLY'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='267' max='267' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [267/267 28:18, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.187400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.726500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.696800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.597400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.551000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.478700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.446000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.454900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.512800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.430300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>2.425000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.361400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>2.437100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.390100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.436600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.383800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>2.333500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.385900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>2.374600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.336300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>2.375900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>2.366000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>2.366800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>2.306600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.396300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>2.342700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 8. Training complete. Saving final model. ---\n",
      "\n",
      "--- All Done ---\n",
      "Your new, 'ablated-and-healed' model (LoRA adapters) is saved in 'llama-3.1-8b-ablated-healed-BENIGN-ONLY'.\n"
     ]
    }
   ],
   "source": [
    "print(\"--- 1. Loading Configuration ---\")\n",
    "\n",
    "model_id = \"./edited_model_TOXIC_ABLATED\"\n",
    "new_model_name = \"llama-3.1-8b-ablated-healed-BENIGN-ONLY\"\n",
    "\n",
    "HASOC_FOLDER = \".\"\n",
    "dataset_files = [\n",
    "    os.path.join(HASOC_FOLDER, \"english_2021.csv\"),\n",
    "    os.path.join(HASOC_FOLDER, \"hindi_2021.csv\"),\n",
    "    os.path.join(HASOC_FOLDER, \"marathi_2021.csv\"),\n",
    "    os.path.join(HASOC_FOLDER, \"english_2019_1.tsv\"),\n",
    "    os.path.join(HASOC_FOLDER, \"english_2019_2.tsv\"),\n",
    "    os.path.join(HASOC_FOLDER, \"hindi_2019_1.tsv\"),\n",
    "    os.path.join(HASOC_FOLDER, \"hindi_2019_2.tsv\"),\n",
    "    os.path.join(HASOC_FOLDER, \"english_2020.xlsx\"),\n",
    "    os.path.join(HASOC_FOLDER, \"hindi_2020.xlsx\")\n",
    "]\n",
    "\n",
    "RESUME_FROM_CHECKPOINT = None\n",
    "\n",
    "print(f\"--- 2. Loading and combining BENIGN-ONLY data (EN, HI, MR) ---\")\n",
    "\n",
    "all_texts = [] \n",
    "total_samples_loaded = 0\n",
    "\n",
    "def load_and_extend(filepath):\n",
    "    text_col, sep = 'text', ','\n",
    "    filetype = filepath.split('.')[-1]\n",
    "    try:\n",
    "        if filetype == 'csv':\n",
    "            df = pd.read_csv(filepath)\n",
    "        elif filetype == 'tsv':\n",
    "            df = pd.read_csv(filepath, sep='\\t', on_bad_lines='skip')\n",
    "        elif filetype == 'xlsx':\n",
    "            df = pd.read_excel(filepath)\n",
    "        if 'text' in df.columns:\n",
    "            text_col = 'text'\n",
    "        else:\n",
    "            print(f\"  Warning: No 'text' column in {filepath}. Skipping file.\")\n",
    "            return 0\n",
    "        if 'task_1' in df.columns:\n",
    "            label_col = 'task_1'\n",
    "        elif 'task1' in df.columns:\n",
    "            label_col = 'task1'\n",
    "        else:\n",
    "            print(f\"  Warning: No 'task_1' or 'task1' in {filepath}. Skipping file.\")\n",
    "            return 0\n",
    "        benign_df = df[df[label_col] == 'NOT']\n",
    "        texts = benign_df[text_col].dropna().tolist()\n",
    "        count = len(texts)\n",
    "        all_texts.extend(texts)\n",
    "        print(f\"  Loaded {count} BENIGN samples from {filepath}.\")\n",
    "        return count\n",
    "    except Exception as e:\n",
    "        print(f\"  Error loading {filepath}: {e}\")\n",
    "        return 0\n",
    "\n",
    "for f in dataset_files:\n",
    "    total_samples_loaded += load_and_extend(f)\n",
    "\n",
    "print(f\"\\nTotal combined multilingual BENIGN dataset size: {total_samples_loaded} samples.\")\n",
    "if total_samples_loaded == 0: \n",
    "    print(\"ERROR: No data was loaded. Check your file paths and formats. Stopping.\")\n",
    "    exit()\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": all_texts})\n",
    "del all_texts\n",
    "\n",
    "print(f\"--- 3. Loading ABLATED model in 4-bit (QLoRA): {model_id} ---\")\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "print(\"Ablated model loaded in 4-bit.\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=512, padding=\"max_length\")\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "print(\"Dataset tokenized and ready.\")\n",
    "\n",
    "print(\"--- 5. Configuring QLoRA ---\")\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(\"QLoRA adapters applied to model.\")\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(\"--- 6. Configuring Trainer ---\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=new_model_name,\n",
    "    per_device_train_batch_size=32,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    save_total_limit=3,\n",
    "    bf16=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(f\"--- 7. Starting 'Healing' Finetuning on BENIGN-ONLY data ---\")\n",
    "print(f\"Checkpoints will be saved to '{new_model_name}'\")\n",
    "\n",
    "trainer.train(resume_from_checkpoint=RESUME_FROM_CHECKPOINT)\n",
    "\n",
    "print(\"--- 8. Training complete. Saving final model. ---\")\n",
    "trainer.save_model(new_model_name)\n",
    "tokenizer.save_pretrained(new_model_name)\n",
    "\n",
    "print(f\"\\n--- All Done ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Loading Configuration ---\n",
      "--- 2. Initializing Perspective API ---\n",
      "  Perspective API client built successfully.\n",
      "--- 3. Loading and randomizing toxic prompts from RTPLX ---\n",
      "Loaded 999 English prompts, randomly sampled 100.\n",
      "Loaded 1115 Hindi prompts, randomly sampled 100.\n",
      "\n",
      "--- 4. Running Generation & Scoring ---\n",
      "\n",
      "--- Processing ORIGINAL Model (meta-llama/Meta-Llama-3.1-8B) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8b3a327f19145ea8233136806944a3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EN Prompts (Baseline):   0%|          | 0/100 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):   1%|          | 1/100 [00:02<03:59,  2.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):   2%|▏         | 2/100 [00:04<03:55,  2.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):   3%|▎         | 3/100 [00:07<03:52,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):   4%|▍         | 4/100 [00:09<03:50,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):   5%|▌         | 5/100 [00:11<03:47,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):   6%|▌         | 6/100 [00:14<03:45,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):   7%|▋         | 7/100 [00:16<03:42,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):   8%|▊         | 8/100 [00:19<03:39,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):   9%|▉         | 9/100 [00:21<03:37,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  10%|█         | 10/100 [00:23<03:35,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  11%|█         | 11/100 [00:26<03:34,  2.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  12%|█▏        | 12/100 [00:28<03:33,  2.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  13%|█▎        | 13/100 [00:31<03:32,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  14%|█▍        | 14/100 [00:33<03:29,  2.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  15%|█▌        | 15/100 [00:36<03:26,  2.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  16%|█▌        | 16/100 [00:38<03:23,  2.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  17%|█▋        | 17/100 [00:40<03:20,  2.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  18%|█▊        | 18/100 [00:43<03:17,  2.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  19%|█▉        | 19/100 [00:45<03:15,  2.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  20%|██        | 20/100 [00:48<03:13,  2.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  21%|██        | 21/100 [00:50<03:10,  2.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  22%|██▏       | 22/100 [00:53<03:07,  2.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  23%|██▎       | 23/100 [00:55<03:04,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  24%|██▍       | 24/100 [00:57<03:02,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  25%|██▌       | 25/100 [01:00<02:59,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  26%|██▌       | 26/100 [01:02<02:57,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  27%|██▋       | 27/100 [01:04<02:54,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  28%|██▊       | 28/100 [01:07<02:51,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  29%|██▉       | 29/100 [01:09<02:48,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  30%|███       | 30/100 [01:12<02:46,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  31%|███       | 31/100 [01:14<02:44,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  32%|███▏      | 32/100 [01:16<02:42,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  33%|███▎      | 33/100 [01:19<02:40,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  34%|███▍      | 34/100 [01:21<02:37,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  35%|███▌      | 35/100 [01:24<02:35,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  36%|███▌      | 36/100 [01:26<02:33,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  37%|███▋      | 37/100 [01:28<02:30,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  38%|███▊      | 38/100 [01:31<02:27,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  39%|███▉      | 39/100 [01:33<02:25,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  40%|████      | 40/100 [01:35<02:23,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  41%|████      | 41/100 [01:38<02:21,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  42%|████▏     | 42/100 [01:40<02:19,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  43%|████▎     | 43/100 [01:43<02:16,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  44%|████▍     | 44/100 [01:45<02:13,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  45%|████▌     | 45/100 [01:47<02:11,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  46%|████▌     | 46/100 [01:50<02:09,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  47%|████▋     | 47/100 [01:52<02:07,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  48%|████▊     | 48/100 [01:55<02:04,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  49%|████▉     | 49/100 [01:57<02:02,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  50%|█████     | 50/100 [02:00<02:00,  2.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  51%|█████     | 51/100 [02:02<01:57,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  52%|█████▏    | 52/100 [02:04<01:55,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  53%|█████▎    | 53/100 [02:07<01:52,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  54%|█████▍    | 54/100 [02:09<01:50,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  55%|█████▌    | 55/100 [02:11<01:47,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  56%|█████▌    | 56/100 [02:14<01:45,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  57%|█████▋    | 57/100 [02:16<01:42,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  58%|█████▊    | 58/100 [02:19<01:40,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  59%|█████▉    | 59/100 [02:21<01:37,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  60%|██████    | 60/100 [02:23<01:35,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  61%|██████    | 61/100 [02:26<01:32,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  62%|██████▏   | 62/100 [02:28<01:30,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  63%|██████▎   | 63/100 [02:31<01:28,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  64%|██████▍   | 64/100 [02:33<01:25,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  65%|██████▌   | 65/100 [02:35<01:23,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  66%|██████▌   | 66/100 [02:38<01:20,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  67%|██████▋   | 67/100 [02:40<01:18,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  68%|██████▊   | 68/100 [02:42<01:16,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  69%|██████▉   | 69/100 [02:45<01:13,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  70%|███████   | 70/100 [02:47<01:11,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  71%|███████   | 71/100 [02:50<01:08,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  72%|███████▏  | 72/100 [02:52<01:06,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  73%|███████▎  | 73/100 [02:54<01:04,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  74%|███████▍  | 74/100 [02:57<01:01,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  75%|███████▌  | 75/100 [02:59<00:59,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  76%|███████▌  | 76/100 [03:01<00:57,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  77%|███████▋  | 77/100 [03:04<00:54,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  78%|███████▊  | 78/100 [03:06<00:52,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  79%|███████▉  | 79/100 [03:09<00:50,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  80%|████████  | 80/100 [03:11<00:47,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  81%|████████  | 81/100 [03:13<00:45,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  82%|████████▏ | 82/100 [03:16<00:42,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  83%|████████▎ | 83/100 [03:18<00:40,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  84%|████████▍ | 84/100 [03:20<00:37,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  85%|████████▌ | 85/100 [03:23<00:35,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  86%|████████▌ | 86/100 [03:25<00:33,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  87%|████████▋ | 87/100 [03:28<00:30,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  88%|████████▊ | 88/100 [03:30<00:28,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  89%|████████▉ | 89/100 [03:32<00:26,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  90%|█████████ | 90/100 [03:35<00:23,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  91%|█████████ | 91/100 [03:37<00:21,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  92%|█████████▏| 92/100 [03:39<00:19,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  93%|█████████▎| 93/100 [03:42<00:16,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  94%|█████████▍| 94/100 [03:44<00:14,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  95%|█████████▌| 95/100 [03:47<00:11,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  96%|█████████▌| 96/100 [03:49<00:09,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  97%|█████████▋| 97/100 [03:51<00:07,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  98%|█████████▊| 98/100 [03:54<00:04,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  99%|█████████▉| 99/100 [03:56<00:02,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline): 100%|██████████| 100/100 [03:59<00:00,  2.39s/it]\n",
      "HI Prompts (Baseline):   0%|          | 0/100 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):   1%|          | 1/100 [00:02<03:56,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):   2%|▏         | 2/100 [00:04<03:52,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):   3%|▎         | 3/100 [00:07<03:50,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):   4%|▍         | 4/100 [00:09<03:48,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):   5%|▌         | 5/100 [00:11<03:46,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):   6%|▌         | 6/100 [00:14<03:43,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):   7%|▋         | 7/100 [00:16<03:41,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):   8%|▊         | 8/100 [00:19<03:39,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):   9%|▉         | 9/100 [00:21<03:37,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  10%|█         | 10/100 [00:23<03:34,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  11%|█         | 11/100 [00:26<03:30,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  12%|█▏        | 12/100 [00:28<03:29,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  13%|█▎        | 13/100 [00:30<03:27,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  14%|█▍        | 14/100 [00:33<03:25,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  15%|█▌        | 15/100 [00:35<03:23,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  16%|█▌        | 16/100 [00:38<03:19,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  17%|█▋        | 17/100 [00:40<03:17,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  18%|█▊        | 18/100 [00:42<03:15,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  19%|█▉        | 19/100 [00:45<03:12,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  20%|██        | 20/100 [00:47<03:10,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  21%|██        | 21/100 [00:50<03:08,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  22%|██▏       | 22/100 [00:52<03:06,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  23%|██▎       | 23/100 [00:54<03:04,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  24%|██▍       | 24/100 [00:57<03:01,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  25%|██▌       | 25/100 [00:59<02:59,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  26%|██▌       | 26/100 [01:01<02:56,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  27%|██▋       | 27/100 [01:04<02:54,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  28%|██▊       | 28/100 [01:06<02:52,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  29%|██▉       | 29/100 [01:09<02:49,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  30%|███       | 30/100 [01:11<02:47,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  31%|███       | 31/100 [01:13<02:45,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  32%|███▏      | 32/100 [01:16<02:42,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  33%|███▎      | 33/100 [01:18<02:40,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  34%|███▍      | 34/100 [01:21<02:36,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  35%|███▌      | 35/100 [01:23<02:34,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  36%|███▌      | 36/100 [01:25<02:32,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  37%|███▋      | 37/100 [01:28<02:30,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  38%|███▊      | 38/100 [01:30<02:28,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  39%|███▉      | 39/100 [01:32<02:16,  2.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  40%|████      | 40/100 [01:34<02:17,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  41%|████      | 41/100 [01:37<02:17,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  42%|████▏     | 42/100 [01:39<02:16,  2.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  43%|████▎     | 43/100 [01:42<02:14,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  44%|████▍     | 44/100 [01:44<02:12,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  45%|████▌     | 45/100 [01:46<02:10,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  46%|████▌     | 46/100 [01:49<02:08,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  47%|████▋     | 47/100 [01:51<02:06,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  48%|████▊     | 48/100 [01:54<02:04,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  49%|████▉     | 49/100 [01:56<02:02,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  50%|█████     | 50/100 [01:58<01:59,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  51%|█████     | 51/100 [02:01<01:57,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  52%|█████▏    | 52/100 [02:03<01:55,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  53%|█████▎    | 53/100 [02:06<01:52,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  54%|█████▍    | 54/100 [02:08<01:49,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  55%|█████▌    | 55/100 [02:10<01:47,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  56%|█████▌    | 56/100 [02:13<01:44,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  57%|█████▋    | 57/100 [02:15<01:42,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  58%|█████▊    | 58/100 [02:17<01:39,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  59%|█████▉    | 59/100 [02:20<01:37,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  60%|██████    | 60/100 [02:22<01:34,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  61%|██████    | 61/100 [02:25<01:32,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  62%|██████▏   | 62/100 [02:27<01:30,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  63%|██████▎   | 63/100 [02:29<01:27,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  64%|██████▍   | 64/100 [02:32<01:25,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  65%|██████▌   | 65/100 [02:34<01:23,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  66%|██████▌   | 66/100 [02:36<01:20,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  67%|██████▋   | 67/100 [02:39<01:18,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  68%|██████▊   | 68/100 [02:41<01:16,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  69%|██████▉   | 69/100 [02:44<01:13,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  70%|███████   | 70/100 [02:46<01:11,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  71%|███████   | 71/100 [02:48<01:08,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  72%|███████▏  | 72/100 [02:51<01:06,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  73%|███████▎  | 73/100 [02:53<01:04,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  74%|███████▍  | 74/100 [02:56<01:01,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  75%|███████▌  | 75/100 [02:58<00:59,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  76%|███████▌  | 76/100 [03:00<00:56,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  77%|███████▋  | 77/100 [03:03<00:54,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  78%|███████▊  | 78/100 [03:05<00:52,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  79%|███████▉  | 79/100 [03:07<00:49,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  80%|████████  | 80/100 [03:10<00:47,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  81%|████████  | 81/100 [03:12<00:45,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  82%|████████▏ | 82/100 [03:14<00:42,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  83%|████████▎ | 83/100 [03:17<00:40,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  84%|████████▍ | 84/100 [03:19<00:38,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  85%|████████▌ | 85/100 [03:22<00:35,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  86%|████████▌ | 86/100 [03:24<00:33,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  87%|████████▋ | 87/100 [03:26<00:31,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  88%|████████▊ | 88/100 [03:29<00:28,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  89%|████████▉ | 89/100 [03:31<00:26,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  90%|█████████ | 90/100 [03:34<00:23,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  91%|█████████ | 91/100 [03:36<00:21,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  92%|█████████▏| 92/100 [03:38<00:19,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  93%|█████████▎| 93/100 [03:41<00:16,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  94%|█████████▍| 94/100 [03:43<00:14,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  95%|█████████▌| 95/100 [03:46<00:11,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  96%|█████████▌| 96/100 [03:48<00:09,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  97%|█████████▋| 97/100 [03:50<00:07,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  98%|█████████▊| 98/100 [03:53<00:04,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  99%|█████████▉| 99/100 [03:55<00:02,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline): 100%|██████████| 100/100 [03:57<00:00,  2.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Clearing VRAM before loading next model ---\n",
      "\n",
      "--- Processing FINETUNED QLoRA Model ---\n",
      "  Loading ablated 4-bit base model from: ./edited_model_TOXIC_ABLATED\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b1c8eefaa1a47b0a73412ebc0aa6d52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loading and merging adapters from: ./llama-3.1-8b-ablated-healed-BENIGN-ONLY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Finetuned model loaded and merged.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EN Prompts (Finetuned):   0%|          | 0/100 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):   1%|          | 1/100 [00:03<04:59,  3.02s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):   2%|▏         | 2/100 [00:05<04:52,  2.99s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):   3%|▎         | 3/100 [00:08<04:49,  2.99s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):   4%|▍         | 4/100 [00:11<04:46,  2.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):   5%|▌         | 5/100 [00:14<04:42,  2.97s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):   6%|▌         | 6/100 [00:17<04:37,  2.96s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):   7%|▋         | 7/100 [00:20<04:35,  2.96s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):   8%|▊         | 8/100 [00:23<04:31,  2.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):   9%|▉         | 9/100 [00:26<04:28,  2.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  10%|█         | 10/100 [00:29<04:24,  2.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  11%|█         | 11/100 [00:32<04:21,  2.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  12%|█▏        | 12/100 [00:34<03:50,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  13%|█▎        | 13/100 [00:37<03:57,  2.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  14%|█▍        | 14/100 [00:40<04:00,  2.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  15%|█▌        | 15/100 [00:43<04:00,  2.83s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  16%|█▌        | 16/100 [00:46<04:00,  2.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  17%|█▋        | 17/100 [00:49<03:58,  2.88s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  18%|█▊        | 18/100 [00:50<03:31,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  19%|█▉        | 19/100 [00:53<03:37,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  20%|██        | 20/100 [00:56<03:41,  2.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  21%|██        | 21/100 [00:59<03:42,  2.81s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  22%|██▏       | 22/100 [01:02<03:42,  2.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  23%|██▎       | 23/100 [01:05<03:41,  2.88s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  24%|██▍       | 24/100 [01:08<03:41,  2.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  25%|██▌       | 25/100 [01:11<03:38,  2.92s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  26%|██▌       | 26/100 [01:14<03:37,  2.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  27%|██▋       | 27/100 [01:17<03:34,  2.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  28%|██▊       | 28/100 [01:19<03:06,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  29%|██▉       | 29/100 [01:22<03:10,  2.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  30%|███       | 30/100 [01:25<03:13,  2.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  31%|███       | 31/100 [01:28<03:13,  2.81s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  32%|███▏      | 32/100 [01:31<03:13,  2.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  33%|███▎      | 33/100 [01:33<03:13,  2.88s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  34%|███▍      | 34/100 [01:36<03:11,  2.90s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  35%|███▌      | 35/100 [01:39<03:09,  2.92s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  36%|███▌      | 36/100 [01:42<03:07,  2.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  37%|███▋      | 37/100 [01:45<03:05,  2.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  38%|███▊      | 38/100 [01:48<03:02,  2.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  39%|███▉      | 39/100 [01:51<02:59,  2.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  40%|████      | 40/100 [01:54<02:56,  2.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  41%|████      | 41/100 [01:57<02:52,  2.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  42%|████▏     | 42/100 [02:00<02:50,  2.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  43%|████▎     | 43/100 [02:03<02:47,  2.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  44%|████▍     | 44/100 [02:06<02:45,  2.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  45%|████▌     | 45/100 [02:09<02:42,  2.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  46%|████▌     | 46/100 [02:12<02:39,  2.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  47%|████▋     | 47/100 [02:15<02:35,  2.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  48%|████▊     | 48/100 [02:18<02:33,  2.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  49%|████▉     | 49/100 [02:21<02:30,  2.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  50%|█████     | 50/100 [02:24<02:27,  2.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  51%|█████     | 51/100 [02:27<02:24,  2.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  52%|█████▏    | 52/100 [02:29<02:21,  2.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  53%|█████▎    | 53/100 [02:32<02:18,  2.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  54%|█████▍    | 54/100 [02:35<02:15,  2.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  55%|█████▌    | 55/100 [02:38<02:12,  2.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  56%|█████▌    | 56/100 [02:41<02:09,  2.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  57%|█████▋    | 57/100 [02:44<02:06,  2.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  58%|█████▊    | 58/100 [02:47<02:02,  2.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  59%|█████▉    | 59/100 [02:50<02:00,  2.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  60%|██████    | 60/100 [02:53<01:57,  2.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  61%|██████    | 61/100 [02:56<01:54,  2.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  62%|██████▏   | 62/100 [02:58<01:40,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  63%|██████▎   | 63/100 [03:01<01:41,  2.74s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  64%|██████▍   | 64/100 [03:04<01:41,  2.81s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  65%|██████▌   | 65/100 [03:07<01:39,  2.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  66%|██████▌   | 66/100 [03:10<01:37,  2.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  67%|██████▋   | 67/100 [03:11<01:23,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  68%|██████▊   | 68/100 [03:14<01:24,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  69%|██████▉   | 69/100 [03:17<01:24,  2.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  70%|███████   | 70/100 [03:20<01:23,  2.78s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  71%|███████   | 71/100 [03:22<01:13,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  72%|███████▏  | 72/100 [03:25<01:13,  2.64s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  73%|███████▎  | 73/100 [03:28<01:13,  2.74s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  74%|███████▍  | 74/100 [03:30<01:02,  2.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  75%|███████▌  | 75/100 [03:33<01:04,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  76%|███████▌  | 76/100 [03:36<01:04,  2.70s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  77%|███████▋  | 77/100 [03:39<01:03,  2.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  78%|███████▊  | 78/100 [03:41<01:02,  2.82s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  79%|███████▉  | 79/100 [03:44<01:00,  2.88s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  80%|████████  | 80/100 [03:46<00:48,  2.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  81%|████████  | 81/100 [03:49<00:49,  2.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  82%|████████▏ | 82/100 [03:52<00:48,  2.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  83%|████████▎ | 83/100 [03:55<00:47,  2.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  84%|████████▍ | 84/100 [03:58<00:45,  2.84s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  85%|████████▌ | 85/100 [04:01<00:43,  2.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  86%|████████▌ | 86/100 [04:04<00:40,  2.89s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  87%|████████▋ | 87/100 [04:07<00:37,  2.92s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  88%|████████▊ | 88/100 [04:10<00:35,  2.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  89%|████████▉ | 89/100 [04:13<00:32,  2.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  90%|█████████ | 90/100 [04:14<00:26,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  91%|█████████ | 91/100 [04:17<00:24,  2.72s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  92%|█████████▏| 92/100 [04:20<00:22,  2.78s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  93%|█████████▎| 93/100 [04:23<00:19,  2.83s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  94%|█████████▍| 94/100 [04:26<00:17,  2.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  95%|█████████▌| 95/100 [04:28<00:12,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  96%|█████████▌| 96/100 [04:29<00:08,  2.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  97%|█████████▋| 97/100 [04:32<00:07,  2.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  98%|█████████▊| 98/100 [04:35<00:05,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  99%|█████████▉| 99/100 [04:38<00:02,  2.70s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned): 100%|██████████| 100/100 [04:41<00:00,  2.82s/it]\n",
      "HI Prompts (Finetuned):   0%|          | 0/100 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):   1%|          | 1/100 [00:02<04:53,  2.96s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):   2%|▏         | 2/100 [00:05<04:49,  2.96s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):   3%|▎         | 3/100 [00:08<04:46,  2.96s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):   4%|▍         | 4/100 [00:11<04:43,  2.96s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):   5%|▌         | 5/100 [00:14<04:40,  2.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):   6%|▌         | 6/100 [00:17<04:37,  2.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):   7%|▋         | 7/100 [00:20<04:33,  2.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):   8%|▊         | 8/100 [00:23<04:31,  2.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):   9%|▉         | 9/100 [00:26<04:27,  2.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  10%|█         | 10/100 [00:29<04:24,  2.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  11%|█         | 11/100 [00:32<04:21,  2.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  13%|█▎        | 13/100 [00:35<03:17,  2.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  14%|█▍        | 14/100 [00:38<03:29,  2.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  15%|█▌        | 15/100 [00:41<03:38,  2.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  16%|█▌        | 16/100 [00:44<03:44,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  17%|█▋        | 17/100 [00:47<03:47,  2.75s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  18%|█▊        | 18/100 [00:50<03:49,  2.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  19%|█▉        | 19/100 [00:53<03:50,  2.84s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  20%|██        | 20/100 [00:56<03:49,  2.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  21%|██        | 21/100 [00:58<03:49,  2.90s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  22%|██▏       | 22/100 [01:01<03:47,  2.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  23%|██▎       | 23/100 [01:04<03:45,  2.92s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  24%|██▍       | 24/100 [01:07<03:42,  2.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  25%|██▌       | 25/100 [01:10<03:40,  2.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  26%|██▌       | 26/100 [01:13<03:37,  2.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  27%|██▋       | 27/100 [01:16<03:34,  2.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  28%|██▊       | 28/100 [01:19<03:32,  2.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  29%|██▉       | 29/100 [01:22<03:29,  2.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  30%|███       | 30/100 [01:25<03:26,  2.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  31%|███       | 31/100 [01:28<03:23,  2.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  32%|███▏      | 32/100 [01:31<03:20,  2.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  33%|███▎      | 33/100 [01:34<03:17,  2.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  34%|███▍      | 34/100 [01:37<03:14,  2.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  35%|███▌      | 35/100 [01:40<03:11,  2.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  36%|███▌      | 36/100 [01:43<03:09,  2.96s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  37%|███▋      | 37/100 [01:46<03:06,  2.96s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  38%|███▊      | 38/100 [01:49<03:02,  2.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  39%|███▉      | 39/100 [01:51<02:46,  2.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  40%|████      | 40/100 [01:54<02:47,  2.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  41%|████      | 41/100 [01:57<02:47,  2.84s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  42%|████▏     | 42/100 [02:00<02:46,  2.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  43%|████▎     | 43/100 [02:03<02:45,  2.90s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  44%|████▍     | 44/100 [02:06<02:43,  2.92s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  45%|████▌     | 45/100 [02:09<02:40,  2.92s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  46%|████▌     | 46/100 [02:12<02:38,  2.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  47%|████▋     | 47/100 [02:14<02:35,  2.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  48%|████▊     | 48/100 [02:17<02:33,  2.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  49%|████▉     | 49/100 [02:20<02:30,  2.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  50%|█████     | 50/100 [02:23<02:27,  2.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  51%|█████     | 51/100 [02:26<02:24,  2.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  52%|█████▏    | 52/100 [02:29<02:21,  2.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  53%|█████▎    | 53/100 [02:32<02:18,  2.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  54%|█████▍    | 54/100 [02:34<02:04,  2.70s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  55%|█████▌    | 55/100 [02:37<02:04,  2.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  56%|█████▌    | 56/100 [02:40<02:04,  2.83s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  57%|█████▋    | 57/100 [02:43<02:03,  2.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  58%|█████▊    | 58/100 [02:46<02:01,  2.89s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  59%|█████▉    | 59/100 [02:49<01:59,  2.90s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  60%|██████    | 60/100 [02:52<01:56,  2.92s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  61%|██████    | 61/100 [02:55<01:53,  2.92s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  62%|██████▏   | 62/100 [02:58<01:51,  2.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  63%|██████▎   | 63/100 [03:01<01:48,  2.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  64%|██████▍   | 64/100 [03:04<01:45,  2.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  65%|██████▌   | 65/100 [03:07<01:43,  2.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  66%|██████▌   | 66/100 [03:10<01:40,  2.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  67%|██████▋   | 67/100 [03:13<01:36,  2.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  68%|██████▊   | 68/100 [03:15<01:33,  2.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  69%|██████▉   | 69/100 [03:18<01:31,  2.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  70%|███████   | 70/100 [03:21<01:28,  2.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  71%|███████   | 71/100 [03:23<01:11,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  72%|███████▏  | 72/100 [03:26<01:13,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  73%|███████▎  | 73/100 [03:29<01:13,  2.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  74%|███████▍  | 74/100 [03:32<01:12,  2.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  75%|███████▌  | 75/100 [03:35<01:11,  2.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  76%|███████▌  | 76/100 [03:38<01:09,  2.88s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  77%|███████▋  | 77/100 [03:41<01:06,  2.90s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  78%|███████▊  | 78/100 [03:44<01:04,  2.92s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  79%|███████▉  | 79/100 [03:46<01:01,  2.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  80%|████████  | 80/100 [03:49<00:58,  2.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  82%|████████▏ | 82/100 [03:52<00:41,  2.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  83%|████████▎ | 83/100 [03:55<00:41,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  84%|████████▍ | 84/100 [03:58<00:41,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  85%|████████▌ | 85/100 [04:01<00:40,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  86%|████████▌ | 86/100 [04:04<00:38,  2.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  87%|████████▋ | 87/100 [04:07<00:36,  2.82s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  88%|████████▊ | 88/100 [04:10<00:34,  2.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  89%|████████▉ | 89/100 [04:13<00:31,  2.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  90%|█████████ | 90/100 [04:16<00:28,  2.90s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  92%|█████████▏| 92/100 [04:19<00:18,  2.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  93%|█████████▎| 93/100 [04:22<00:16,  2.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  94%|█████████▍| 94/100 [04:25<00:15,  2.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  95%|█████████▌| 95/100 [04:28<00:13,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  96%|█████████▌| 96/100 [04:31<00:10,  2.74s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  97%|█████████▋| 97/100 [04:34<00:08,  2.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  98%|█████████▊| 98/100 [04:37<00:05,  2.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  99%|█████████▉| 99/100 [04:40<00:02,  2.88s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned): 100%|██████████| 100/100 [04:43<00:00,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "--- FINAL RESULTS (Average Toxicity) ---\n",
      "\n",
      "--- ENGLISH Test (100 Prompts) ---\n",
      "  Average Baseline Toxicity:  0.4247\n",
      "  Average Finetuned Toxicity: 0.4164\n",
      "    (Reduction: 0.0082)\n",
      "\n",
      "--- HINDI Test (100 Prompts) ---\n",
      "  Average Baseline Toxicity:  0.3957\n",
      "  Average Finetuned Toxicity: 0.2787\n",
      "    (Reduction: 0.1171)\n",
      "\n",
      "--- MIXED (Average) Test (200 Prompts) ---\n",
      "  Average Baseline Toxicity:  0.4102\n",
      "  Average Finetuned Toxicity: 0.3475\n",
      "    (Reduction: 0.0627)\n",
      "\n",
      "--- Validation complete. ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"--- 1. Loading Configuration ---\")\n",
    "\n",
    "model_id_original = \"meta-llama/Meta-Llama-3.1-8B\"\n",
    "model_id_ablated_base = \"./edited_model_TOXIC_ABLATED\"\n",
    "model_id_finetuned_adapters = \"./llama-3.1-8b-ablated-healed-BENIGN-ONLY\"\n",
    "\n",
    "rtplx_folder = \"rtplx\"\n",
    "rtplx_en_file = \"RTP_LX_EN.json\"\n",
    "rtplx_hi_file = \"RTP_LX_HI.json\"\n",
    "\n",
    "NUM_PROMPTS_TO_TEST = 100\n",
    "\n",
    "print(\"--- 2. Initializing Perspective API ---\")\n",
    "try:\n",
    "    perspective = build(\n",
    "        'commentanalyzer',\n",
    "        'v1alpha1',\n",
    "        developerKey=API_KEY,\n",
    "        discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
    "        static_discovery=False\n",
    "    )\n",
    "    print(\"  Perspective API client built successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Could not initialize Perspective API: {e}\")\n",
    "    perspective = None\n",
    "\n",
    "def get_toxicity_score(text, lang='en'):\n",
    "    if perspective is None:\n",
    "        return None\n",
    "    if not text.strip() or \"bool]bool\" in text or \"otasotas\" in text:\n",
    "        return 0.0\n",
    "    time.sleep(1.2)\n",
    "    analyze_request = {\n",
    "        'comment': {'text': text},\n",
    "        'requestedAttributes': {'TOXICITY': {}},\n",
    "        'languages': [lang]\n",
    "    }\n",
    "    try:\n",
    "        response = perspective.comments().analyze(body=analyze_request).execute()\n",
    "        score = response['attributeScores']['TOXICITY']['summaryScore']['value']\n",
    "        return round(score, 4)\n",
    "    except HttpError as e:\n",
    "        if e.resp.status == 429:\n",
    "            print(\"  [Perspective API Error: Quota exceeded. Sleeping for 5s...]\")\n",
    "            time.sleep(5)\n",
    "            return get_toxicity_score(text, lang)\n",
    "        print(f\"  [Perspective API Error for lang={lang}: {e}]\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"  [Unexpected API Error for lang={lang}: {e}]\")\n",
    "        return None\n",
    "\n",
    "print(f\"--- 3. Loading and randomizing toxic prompts from RTPLX ---\")\n",
    "toxic_prompts_en, toxic_prompts_hi = [], []\n",
    "try:\n",
    "    filepath_en = os.path.join(rtplx_folder, rtplx_en_file)\n",
    "    all_en_prompts = [json.loads(line)[\"Prompt\"] for line in open(filepath_en, 'r', encoding='utf-8')]\n",
    "    num_to_sample_en = min(NUM_PROMPTS_TO_TEST, len(all_en_prompts))\n",
    "    toxic_prompts_en = random.sample(all_en_prompts, num_to_sample_en)\n",
    "    print(f\"Loaded {len(all_en_prompts)} English prompts, randomly sampled {len(toxic_prompts_en)}.\")\n",
    "    del all_en_prompts\n",
    "\n",
    "    filepath_hi = os.path.join(rtplx_folder, rtplx_hi_file)\n",
    "    all_hi_prompts = [json.loads(line)[\"Prompt\"] for line in open(filepath_hi, 'r', encoding='utf-8')]\n",
    "    num_to_sample_hi = min(NUM_PROMPTS_TO_TEST, len(all_hi_prompts))\n",
    "    toxic_prompts_hi = random.sample(all_hi_prompts, num_to_sample_hi)\n",
    "    print(f\"Loaded {len(all_hi_prompts)} Hindi prompts, randomly sampled {len(toxic_prompts_hi)}.\")\n",
    "    del all_hi_prompts\n",
    "except Exception as e:\n",
    "    print(f\"Error loading test prompts: {e}\")\n",
    "    exit()\n",
    "\n",
    "print(f\"\\n--- 4. Running Generation & Scoring ---\")\n",
    "generation_kwargs = {\"max_new_tokens\": 60, \"do_sample\": False}\n",
    "all_scores = {\n",
    "    \"baseline_en\": [], \"finetuned_en\": [],\n",
    "    \"baseline_hi\": [], \"finetuned_hi\": []\n",
    "}\n",
    "\n",
    "print(f\"\\n--- Processing ORIGINAL Model ({model_id_original}) ---\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id_original, token=my_token)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id_original, dtype=torch.bfloat16, device_map=\"auto\", token=my_token\n",
    "    )\n",
    "    model.eval()\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"left\"\n",
    "\n",
    "    for prompt in tqdm(toxic_prompts_en, desc=\"EN Prompts (Baseline)\"):\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(**inputs, **generation_kwargs)\n",
    "        text = tokenizer.decode(output[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "        score = get_toxicity_score(text, lang='en')\n",
    "        if score is not None:\n",
    "            all_scores[\"baseline_en\"].append(score)\n",
    "\n",
    "    for prompt in tqdm(toxic_prompts_hi, desc=\"HI Prompts (Baseline)\"):\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(**inputs, **generation_kwargs)\n",
    "        text = tokenizer.decode(output[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "        score = get_toxicity_score(text, lang='hi')\n",
    "        if score is not None:\n",
    "            all_scores[\"baseline_hi\"].append(score)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during Baseline model testing: {e}\")\n",
    "\n",
    "print(\"--- Clearing VRAM before loading next model ---\")\n",
    "del model\n",
    "del tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\n--- Processing FINETUNED QLoRA Model ---\")\n",
    "try:\n",
    "    print(f\"  Loading ablated 4-bit base model from: {model_id_ablated_base}\")\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id_ablated_base,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id_ablated_base)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"left\"\n",
    "\n",
    "    print(f\"  Loading and merging adapters from: {model_id_finetuned_adapters}\")\n",
    "    model = PeftModel.from_pretrained(model, model_id_finetuned_adapters)\n",
    "    model = model.merge_and_unload()\n",
    "    print(\"  Finetuned model loaded and merged.\")\n",
    "    model.eval()\n",
    "\n",
    "    for prompt in tqdm(toxic_prompts_en, desc=\"EN Prompts (Finetuned)\"):\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(**inputs, **generation_kwargs)\n",
    "        text = tokenizer.decode(output[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "        score = get_toxicity_score(text, lang='en')\n",
    "        if score is not None:\n",
    "            all_scores[\"finetuned_en\"].append(score)\n",
    "\n",
    "    for prompt in tqdm(toxic_prompts_hi, desc=\"HI Prompts (Finetuned)\"):\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(**inputs, **generation_kwargs)\n",
    "        text = tokenizer.decode(output[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "        score = get_toxicity_score(text, lang='hi')\n",
    "        if score is not None:\n",
    "            all_scores[\"finetuned_hi\"].append(score)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during Finetuned model testing: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"--- FINAL RESULTS (Average Toxicity) ---\")\n",
    "\n",
    "def get_avg_score(key):\n",
    "    if all_scores[key]:\n",
    "        return np.mean(all_scores[key])\n",
    "    return 0.0\n",
    "\n",
    "avg_baseline_en = get_avg_score(\"baseline_en\")\n",
    "avg_finetuned_en = get_avg_score(\"finetuned_en\")\n",
    "avg_baseline_hi = get_avg_score(\"baseline_hi\")\n",
    "avg_finetuned_hi = get_avg_score(\"finetuned_hi\")\n",
    "\n",
    "avg_baseline_mix = np.mean([avg_baseline_en, avg_baseline_hi]) if all_scores[\"baseline_en\"] and all_scores[\"baseline_hi\"] else 0\n",
    "avg_finetuned_mix = np.mean([avg_finetuned_en, avg_finetuned_hi]) if all_scores[\"finetuned_en\"] and all_scores[\"finetuned_hi\"] else 0\n",
    "\n",
    "def print_summary(name, baseline, finetuned, count):\n",
    "    print(f\"\\n--- {name} Test ({count} Prompts) ---\")\n",
    "    reduction = baseline - finetuned\n",
    "    print(f\"  Average Baseline Toxicity:  {baseline:.4f}\")\n",
    "    print(f\"  Average Finetuned Toxicity: {finetuned:.4f}\")\n",
    "    print(f\"    (Reduction: {reduction:.4f})\")\n",
    "\n",
    "print_summary(\"ENGLISH\", avg_baseline_en, avg_finetuned_en, len(all_scores[\"baseline_en\"]))\n",
    "print_summary(\"HINDI\", avg_baseline_hi, avg_finetuned_hi, len(all_scores[\"baseline_hi\"]))\n",
    "print_summary(\"MIXED (Average)\", avg_baseline_mix, avg_finetuned_mix, len(all_scores[\"baseline_en\"]) + len(all_scores[\"baseline_hi\"]))\n",
    "\n",
    "print(\"\\n--- Validation complete. ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
