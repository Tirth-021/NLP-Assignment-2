{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agnostic-Only Finetuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install --upgrade --force-reinstall google-api-python-client\n",
    "!pip install --upgrade tensorboard\n",
    "!pip install accelerate\n",
    "!pip install google-api-python-client\n",
    "!pip install numpy\n",
    "!pip install langid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"GoogleCloudAPI\" \n",
    "my_token = \"HuggingFaceToken\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Preprocessing HASOC CSVs (from .) ---\n",
      "  Processing ./english_2021.csv...\n",
      "    Wrote 1342 benign samples to temp_jsonl_data/hasoc_EN_benign.jsonl\n",
      "  Processing ./hindi_2021.csv...\n",
      "    Wrote 3161 benign samples to temp_jsonl_data/hasoc_HI_benign.jsonl\n",
      "  Processing ./marathi_2021.csv...\n",
      "    Wrote 1205 benign samples to temp_jsonl_data/hasoc_MR_benign.jsonl\n",
      "Preprocessing complete.\n",
      "\n",
      "--- 2. Loading Model and Tokenizer ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c2715d5c2404c3098e2420037b3a31b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08e6e56871f440cc87970ada85f16bf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3fa74a199d942f299afe33ac0ad5ec5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6295690a552a4955897715e41406c503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/826 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fc274db67f14f58b263de3c9db0870b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e318b00e20b74706a7ea167dbeddb934",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72e08d3e38af4b49b6101dc01fd24ab2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36707e64526247a89eb6f9e9691a28e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee6cbea38b424f1ab66bce3246aed999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6e8baab03f24ceabc563c28c6cf4b32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e998f21f1ed64aa4b684a54fd6f1587b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b76c07e444040cc85573c4e878adc5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n",
      "\n",
      "\n",
      "--- 4. Starting LAPE Collection & Analysis Loop ---\n",
      "\n",
      "  --- Language-Agnostic Analysis (EN vs HI vs MR) ---\n",
      "    Running Benign EN collection...\n",
      "    Loaded 1342 prompts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Running Benign HI collection...\n",
      "    Loaded 3161 prompts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Running Benign MR collection...\n",
      "    Loaded 1205 prompts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Analyzing (EN vs HI vs MR) for high-entropy neurons...\n",
      "      Analyzing with LAPE (for High Entropy)...\n",
      "    Language-agnostic analysis complete.\n",
      "    Cleaning up all activations from memory.\n",
      "\n",
      "--- 5. All processing complete. Saving final results. ---\n",
      "LAPE (Agnostic) analysis successfully saved to 'agnostic_neurons_LAPE.json'.\n",
      "\n",
      "--- All Done ---\n"
     ]
    }
   ],
   "source": [
    "HASOC_FOLDER = \".\"  \n",
    "TEMP_DATA_FOLDER = \"temp_jsonl_data\" \n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B\"\n",
    "\n",
    "batch_size = 64\n",
    "max_length = 400 \n",
    "\n",
    "TOP_N_NEURONS = 100 \n",
    "RESULTS_FILE = \"agnostic_neurons_LAPE.json\" \n",
    "ACTIVATION_THRESHOLD = 0.0\n",
    "EPSILON = 1e-9 \n",
    "\n",
    "print(f\"--- 1. Preprocessing HASOC CSVs (from {HASOC_FOLDER}) ---\")\n",
    "os.makedirs(TEMP_DATA_FOLDER, exist_ok=True)\n",
    "\n",
    "csv_files_to_process = {\n",
    "    \"EN\": os.path.join(HASOC_FOLDER, \"english_2021.csv\"),\n",
    "    \"HI\": os.path.join(HASOC_FOLDER, \"hindi_2021.csv\"),\n",
    "    \"MR\": os.path.join(HASOC_FOLDER, \"marathi_2021.csv\")\n",
    "}\n",
    "\n",
    "job_file_map = {}\n",
    "\n",
    "for lang, filepath in csv_files_to_process.items():\n",
    "    print(f\"  Processing {filepath}...\")\n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        \n",
    "        if lang == 'MR':\n",
    "            text_col = 'text'\n",
    "            label_col = 'task_1'\n",
    "        else: \n",
    "            text_col = 'text'\n",
    "            label_col = 'task_1'\n",
    "\n",
    "        benign_texts = df[df[label_col] == 'NOT'][text_col].tolist()\n",
    "        \n",
    "        benign_outfile = os.path.join(TEMP_DATA_FOLDER, f\"hasoc_{lang}_benign.jsonl\")\n",
    "        job_file_map[f\"benign_{lang}\"] = (benign_outfile, \"BenignCompletion\")\n",
    "        \n",
    "        with open(benign_outfile, 'w', encoding='utf-8') as f:\n",
    "            for text in benign_texts:\n",
    "                json.dump({\"BenignCompletion\": text}, f)\n",
    "                f.write('\\n')\n",
    "        print(f\"    Wrote {len(benign_texts)} benign samples to {benign_outfile}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"    ERROR: File not found at {filepath}. Please check your paths.\")\n",
    "    except Exception as e:\n",
    "        print(f\"    An error occurred processing {filepath}: {e}\")\n",
    "\n",
    "print(\"Preprocessing complete.\\n\")\n",
    "\n",
    "\n",
    "print(\"--- 2. Loading Model and Tokenizer ---\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=my_token)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "print(\"Tokenizer loaded.\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, dtype=torch.bfloat16, device_map=\"auto\", token=my_token\n",
    ")\n",
    "model.eval()\n",
    "print(\"Model loaded successfully.\\n\")\n",
    "\n",
    "\n",
    "def get_hook(storage_dict, layer_name):\n",
    "    def hook_fn(module, input, output):\n",
    "        storage_dict[layer_name].append(output.detach().cpu())\n",
    "    return hook_fn\n",
    "\n",
    "def run_activation_collection(input_filename, data_field, layer_names):\n",
    "    all_prompts = []\n",
    "    try:\n",
    "        with open(input_filename, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line)\n",
    "                all_prompts.append(data[data_field])\n",
    "        print(f\"    Loaded {len(all_prompts)} prompts.\")\n",
    "    except Exception as e:\n",
    "        print(f\"    Error loading {input_filename}: {e}\")\n",
    "        return None\n",
    "\n",
    "    activation_storage = {} \n",
    "    hook_handles = []\n",
    "    for layer_name in layer_names:\n",
    "        activation_storage[layer_name] = []\n",
    "    \n",
    "    for i, layer in enumerate(model.model.layers):\n",
    "        if i >= len(layer_names): break\n",
    "        handle = layer.mlp.register_forward_hook(\n",
    "            get_hook(activation_storage, layer_names[i])\n",
    "        )\n",
    "        hook_handles.append(handle)\n",
    "\n",
    "    all_collected_activations = {name: [] for name in layer_names}\n",
    "    num_batches = math.ceil(len(all_prompts) / batch_size)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(all_prompts), batch_size), desc=\"      Processing\", ncols=100, leave=False):\n",
    "            for layer_name in activation_storage:\n",
    "                activation_storage[layer_name].clear()\n",
    "            \n",
    "            batch_prompts = all_prompts[i:i+batch_size]\n",
    "            \n",
    "            inputs = tokenizer(\n",
    "                batch_prompts, return_tensors=\"pt\", padding=\"max_length\",\n",
    "                truncation=True, max_length=max_length\n",
    "            ).to(\"cuda\")\n",
    "            \n",
    "            model(**inputs)\n",
    "            \n",
    "            last_token_indices = (inputs[\"attention_mask\"].sum(dim=1) - 1).cpu()\n",
    "\n",
    "            for layer_name, batch_activations_list in activation_storage.items():\n",
    "                full_batch_tensor = batch_activations_list[0] \n",
    "                last_token_activations = full_batch_tensor[\n",
    "                    torch.arange(full_batch_tensor.size(0)),\n",
    "                    last_token_indices\n",
    "                ] \n",
    "                all_collected_activations[layer_name].append(last_token_activations)\n",
    "    \n",
    "    for handle in hook_handles:\n",
    "        handle.remove()\n",
    "    \n",
    "    final_activations = {}\n",
    "    for layer_name, tensor_list in all_collected_activations.items():\n",
    "        if tensor_list:\n",
    "            final_activations[layer_name] = torch.cat(tensor_list, dim=0)\n",
    "            \n",
    "    del all_prompts, all_collected_activations, activation_storage, hook_handles\n",
    "    gc.collect()\n",
    "    \n",
    "    return final_activations\n",
    "\n",
    "\n",
    "def analyze_with_lape_3_lang(acts_en, acts_hi, acts_mr, layer_names):\n",
    "\n",
    "    print(\"      Analyzing with LAPE (for High Entropy)...\")\n",
    "    layer_results = {}\n",
    "    for layer_name in layer_names:\n",
    "        tensor_en = acts_en[layer_name] \n",
    "        tensor_hi = acts_hi[layer_name] \n",
    "        tensor_mr = acts_mr[layer_name] \n",
    "\n",
    "        # 1. Binarize: Did the neuron fire? (act > 0)\n",
    "        binarized_en = tensor_en > ACTIVATION_THRESHOLD\n",
    "        binarized_hi = tensor_hi > ACTIVATION_THRESHOLD\n",
    "        binarized_mr = tensor_mr > ACTIVATION_THRESHOLD\n",
    "        \n",
    "        # 2. Calculate Firing Probability (P_L) for each language\n",
    "        P_en = binarized_en.float().mean(dim=0)\n",
    "        P_hi = binarized_hi.float().mean(dim=0)\n",
    "        P_mr = binarized_mr.float().mean(dim=0)\n",
    "        \n",
    "        # 3. Normalize Probabilities (p_L)\n",
    "        total_prob = P_en + P_hi + P_mr\n",
    "        total_prob[total_prob == 0] = 1.0 \n",
    "        \n",
    "        p_en = P_en / total_prob\n",
    "        p_hi = P_hi / total_prob\n",
    "        p_mr = P_mr / total_prob\n",
    "        \n",
    "        # 4. Calculate Entropy (H)\n",
    "        log_p_en = torch.log2(p_en + EPSILON)\n",
    "        log_p_hi = torch.log2(p_hi + EPSILON)\n",
    "        log_p_mr = torch.log2(p_mr + EPSILON)\n",
    "        \n",
    "        entropy = - (p_en * log_p_en + p_hi * log_p_hi + p_mr * log_p_mr)\n",
    "        \n",
    "        # 5. Find High-Entropy (Agnostic) Neurons\n",
    "        top_values, top_indices = torch.topk(\n",
    "            entropy, TOP_N_NEURONS, largest=True \n",
    "        )\n",
    "        \n",
    "        layer_results[layer_name] = {\n",
    "            \"agnostic_neuron_indices\": top_indices.cpu().numpy().tolist(),\n",
    "            \"entropy_scores\": top_values.cpu().float().numpy().tolist()\n",
    "        }\n",
    "    return layer_results\n",
    "\n",
    "print(\"\\n--- 4. Starting LAPE Collection & Analysis Loop ---\")\n",
    "\n",
    "layer_names = [f\"model.model.layers.{i}.mlp\" for i in range(len(model.model.layers))]\n",
    "final_analysis_results = {}\n",
    "\n",
    "print(\"\\n  --- Language-Agnostic Analysis (EN vs HI vs MR) ---\")\n",
    "print(\"    Running Benign EN collection...\")\n",
    "benign_file_en, benign_field_en = job_file_map[\"benign_EN\"]\n",
    "benign_activations_en = run_activation_collection(benign_file_en, benign_field_en, layer_names)\n",
    "\n",
    "print(\"    Running Benign HI collection...\")\n",
    "benign_file_hi, benign_field_hi = job_file_map[\"benign_HI\"]\n",
    "benign_activations_hi = run_activation_collection(benign_file_hi, benign_field_hi, layer_names)\n",
    "\n",
    "print(\"    Running Benign MR collection...\")\n",
    "benign_file_mr, benign_field_mr = job_file_map[\"benign_MR\"]\n",
    "benign_activations_mr = run_activation_collection(benign_file_mr, benign_field_mr, layer_names)\n",
    "\n",
    "if benign_activations_en and benign_activations_hi and benign_activations_mr:\n",
    "    print(\"    Analyzing (EN vs HI vs MR) for high-entropy neurons...\")\n",
    "    final_analysis_results[\"language_agnostic_neurons\"] = analyze_with_lape_3_lang(\n",
    "        benign_activations_en, benign_activations_hi, benign_activations_mr, layer_names\n",
    "    )\n",
    "    print(\"    Language-agnostic analysis complete.\")\n",
    "else:\n",
    "    print(\"    ERROR: Failed to load activations for one or more languages. Skipping analysis.\")\n",
    "\n",
    "print(\"    Cleaning up all activations from memory.\")\n",
    "del benign_activations_en\n",
    "del benign_activations_hi\n",
    "del benign_activations_mr\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\n--- 5. All processing complete. Saving final results. ---\")\n",
    "try:\n",
    "    with open(RESULTS_FILE, 'w') as f:\n",
    "        json.dump(final_analysis_results, f, indent=2)\n",
    "    print(f\"LAPE (Agnostic) analysis successfully saved to '{RESULTS_FILE}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Could not save final results: {e}\")\n",
    "\n",
    "print(\"\\n--- All Done ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing cache and collecting garbage...\n",
      "VRAM cache cleared.\n",
      "--- 1. Loading Configuration ---\n",
      "--- 2. Loading Agnostic Neurons from agnostic_neurons_LAPE.json ---\n",
      "Loaded agnostic neuron indices for 32 layers.\n",
      "--- 3. Loading original model in bfloat16: meta-llama/Meta-Llama-3.1-8B ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23440629fd37469f8ce7fd171fb1e892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model loaded.\n",
      "--- 4. Loading and combining BENIGN-ONLY data (EN, HI, MR) ---\n",
      "  Loaded 1342 BENIGN samples from ./english_2021.csv.\n",
      "  Loaded 3161 BENIGN samples from ./hindi_2021.csv.\n",
      "  Loaded 1205 BENIGN samples from ./marathi_2021.csv.\n",
      "  Loaded 3591 BENIGN samples from ./english_2019_1.tsv.\n",
      "  Loaded 865 BENIGN samples from ./english_2019_2.tsv.\n",
      "  Loaded 713 BENIGN samples from ./hindi_2019_1.tsv.\n",
      "  Loaded 2196 BENIGN samples from ./hindi_2019_2.tsv.\n",
      "  Loaded 1852 BENIGN samples from ./english_2020.xlsx.\n",
      "  Loaded 2116 BENIGN samples from ./hindi_2020.xlsx.\n",
      "\n",
      "Total combined multilingual BENIGN dataset size: 17041 samples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f51f91109664fa184976ccb0bedd7aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/17041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset tokenized and ready.\n",
      "--- 5. Applying gradient mask to finetune *only* agnostic neurons ---\n",
      "  Mask applied to model.model.layers.0.mlp. 100 neurons will be trained.\n",
      "  Mask applied to model.model.layers.1.mlp. 100 neurons will be trained.\n",
      "  Mask applied to model.model.layers.2.mlp. 100 neurons will be trained.\n",
      "  Mask applied to model.model.layers.3.mlp. 100 neurons will be trained.\n",
      "  Mask applied to model.model.layers.4.mlp. 100 neurons will be trained.\n",
      "--- 6. Configuring Trainer ---\n",
      "--- 7. Starting Finetuning on BENIGN-ONLY data ---\n",
      "Checkpoints will be saved to 'llama-3.1-8b-agnostic-finetuned-BENIGN-ONLY'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/nn/modules/module.py:1847: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.CausalLMOutputWithPast'>\n",
      "  warnings.warn(\"For backward hooks to be called,\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1599' max='1599' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1599/1599 55:26, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.662900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.570600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.549300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.376100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.511900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.472200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.450800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.380600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.335100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.458000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>2.332000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.343900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>2.323500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.305100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.342600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.343800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>2.436700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.368800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>2.392100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.262600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>2.346500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>2.289900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>2.231600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>2.280500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.311500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>2.363000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>2.283300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>2.292300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>2.326000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.343300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>2.266400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>2.307500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>2.261700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>2.204200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.262000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>2.322700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>2.263500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>2.277300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>2.252300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.205900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>2.301100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>2.269500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>2.271700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>2.266900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>2.293200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>2.228900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>2.176200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>2.249100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>2.248700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.342800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>2.236200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>2.241700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>2.167500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>2.137700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.959400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>2.013600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>1.935300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>1.929700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>1.938900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.013800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>1.984600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>2.025100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>1.982500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>1.909500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.976900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>1.932100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>2.008200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>1.900600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>1.996200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.946000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>1.855200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>1.911700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>2.022200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>1.906000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>2.043500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>1.991300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>1.808100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>2.028200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>1.852900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.970400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>1.952400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>1.954800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>1.952400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>1.977100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.974700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>1.971200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>1.934600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>1.834300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>1.842200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.023600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>1.997400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>1.982700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>1.957600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>1.994800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.987500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>1.990700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>1.896900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>1.950700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>1.967800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.993000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>2.024600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>1.948500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>1.953400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>1.905600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.856700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>1.922900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>1.901100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>1.785900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>1.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.785900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>1.851400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>1.828200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>1.768900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>1.805800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>1.729100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>1.830200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>1.809100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>1.764600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>1.781600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.866500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210</td>\n",
       "      <td>1.741800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>1.802400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1230</td>\n",
       "      <td>1.746000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>1.854000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>1.796300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>1.744800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1270</td>\n",
       "      <td>1.886800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>1.814600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1290</td>\n",
       "      <td>1.790600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.714200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1310</td>\n",
       "      <td>1.738300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>1.734900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1330</td>\n",
       "      <td>1.730700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>1.873200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>1.907600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>1.852800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1370</td>\n",
       "      <td>1.792700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>1.820100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1390</td>\n",
       "      <td>1.856600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.890700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1410</td>\n",
       "      <td>1.909500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>1.837600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1430</td>\n",
       "      <td>1.756600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>1.767000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>1.753600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>1.779800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1470</td>\n",
       "      <td>1.856900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>1.731800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1490</td>\n",
       "      <td>1.852100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.700100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1510</td>\n",
       "      <td>1.802800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>1.724600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1530</td>\n",
       "      <td>1.807200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1540</td>\n",
       "      <td>1.715100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>1.837400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>1.764200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1570</td>\n",
       "      <td>1.850800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1580</td>\n",
       "      <td>1.886400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1590</td>\n",
       "      <td>1.809600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/nn/modules/module.py:1847: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.CausalLMOutputWithPast'>\n",
      "  warnings.warn(\"For backward hooks to be called,\"\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/nn/modules/module.py:1847: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.CausalLMOutputWithPast'>\n",
      "  warnings.warn(\"For backward hooks to be called,\"\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/nn/modules/module.py:1847: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.CausalLMOutputWithPast'>\n",
      "  warnings.warn(\"For backward hooks to be called,\"\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/nn/modules/module.py:1847: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.CausalLMOutputWithPast'>\n",
      "  warnings.warn(\"For backward hooks to be called,\"\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/nn/modules/module.py:1847: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.CausalLMOutputWithPast'>\n",
      "  warnings.warn(\"For backward hooks to be called,\"\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/nn/modules/module.py:1847: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.CausalLMOutputWithPast'>\n",
      "  warnings.warn(\"For backward hooks to be called,\"\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/nn/modules/module.py:1847: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.CausalLMOutputWithPast'>\n",
      "  warnings.warn(\"For backward hooks to be called,\"\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/nn/modules/module.py:1847: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.CausalLMOutputWithPast'>\n",
      "  warnings.warn(\"For backward hooks to be called,\"\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/nn/modules/module.py:1847: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.CausalLMOutputWithPast'>\n",
      "  warnings.warn(\"For backward hooks to be called,\"\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/nn/modules/module.py:1847: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.CausalLMOutputWithPast'>\n",
      "  warnings.warn(\"For backward hooks to be called,\"\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/nn/modules/module.py:1847: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.CausalLMOutputWithPast'>\n",
      "  warnings.warn(\"For backward hooks to be called,\"\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/nn/modules/module.py:1847: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.CausalLMOutputWithPast'>\n",
      "  warnings.warn(\"For backward hooks to be called,\"\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/nn/modules/module.py:1847: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.CausalLMOutputWithPast'>\n",
      "  warnings.warn(\"For backward hooks to be called,\"\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/nn/modules/module.py:1847: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.CausalLMOutputWithPast'>\n",
      "  warnings.warn(\"For backward hooks to be called,\"\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/nn/modules/module.py:1847: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.CausalLMOutputWithPast'>\n",
      "  warnings.warn(\"For backward hooks to be called,\"\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/nn/modules/module.py:1847: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.CausalLMOutputWithPast'>\n",
      "  warnings.warn(\"For backward hooks to be called,\"\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/nn/modules/module.py:1847: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.CausalLMOutputWithPast'>\n",
      "  warnings.warn(\"For backward hooks to be called,\"\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/nn/modules/module.py:1847: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.CausalLMOutputWithPast'>\n",
      "  warnings.warn(\"For backward hooks to be called,\"\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/nn/modules/module.py:1847: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.CausalLMOutputWithPast'>\n",
      "  warnings.warn(\"For backward hooks to be called,\"\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/nn/modules/module.py:1847: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.CausalLMOutputWithPast'>\n",
      "  warnings.warn(\"For backward hooks to be called,\"\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/nn/modules/module.py:1847: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.CausalLMOutputWithPast'>\n",
      "  warnings.warn(\"For backward hooks to be called,\"\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/nn/modules/module.py:1847: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.CausalLMOutputWithPast'>\n",
      "  warnings.warn(\"For backward hooks to be called,\"\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/nn/modules/module.py:1847: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.CausalLMOutputWithPast'>\n",
      "  warnings.warn(\"For backward hooks to be called,\"\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/nn/modules/module.py:1847: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.CausalLMOutputWithPast'>\n",
      "  warnings.warn(\"For backward hooks to be called,\"\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/nn/modules/module.py:1847: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.CausalLMOutputWithPast'>\n",
      "  warnings.warn(\"For backward hooks to be called,\"\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/nn/modules/module.py:1847: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.CausalLMOutputWithPast'>\n",
      "  warnings.warn(\"For backward hooks to be called,\"\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/nn/modules/module.py:1847: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.CausalLMOutputWithPast'>\n",
      "  warnings.warn(\"For backward hooks to be called,\"\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/nn/modules/module.py:1847: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.CausalLMOutputWithPast'>\n",
      "  warnings.warn(\"For backward hooks to be called,\"\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/nn/modules/module.py:1847: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.CausalLMOutputWithPast'>\n",
      "  warnings.warn(\"For backward hooks to be called,\"\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/nn/modules/module.py:1847: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.CausalLMOutputWithPast'>\n",
      "  warnings.warn(\"For backward hooks to be called,\"\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/nn/modules/module.py:1847: UserWarning: For backward hooks to be called, module output should be a Tensor or a tuple of Tensors but received <class 'transformers.modeling_outputs.CausalLMOutputWithPast'>\n",
      "  warnings.warn(\"For backward hooks to be called,\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 8. Training complete. Saving final model. ---\n",
      "\n",
      "--- All Done ---\n",
      "Your new, benign-finetuned model is saved in 'llama-3.1-8b-agnostic-finetuned-BENIGN-ONLY'.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import gc\n",
    "\n",
    "print(\"Clearing cache and collecting garbage...\")\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"VRAM cache cleared.\")\n",
    "\n",
    "print(\"--- 1. Loading Configuration ---\")\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B\"\n",
    "analysis_file = \"agnostic_neurons_LAPE.json\"\n",
    "new_model_name = \"llama-3.1-8b-agnostic-finetuned-BENIGN-ONLY\"\n",
    "\n",
    "HASOC_FOLDER = \".\"\n",
    "hasoc_en_csv_2021 = os.path.join(HASOC_FOLDER, \"english_2021.csv\")\n",
    "hasoc_hi_csv_2021 = os.path.join(HASOC_FOLDER, \"hindi_2021.csv\")\n",
    "hasoc_mr_csv_2021 = os.path.join(HASOC_FOLDER, \"marathi_2021.csv\")\n",
    "hasoc_en_tsv_2019_1 = os.path.join(HASOC_FOLDER, \"english_2019_1.tsv\")\n",
    "hasoc_en_tsv_2019_2 = os.path.join(HASOC_FOLDER, \"english_2019_2.tsv\")\n",
    "hasoc_hi_tsv_2019_1 = os.path.join(HASOC_FOLDER, \"hindi_2019_1.tsv\")\n",
    "hasoc_hi_tsv_2019_2 = os.path.join(HASOC_FOLDER, \"hindi_2019_2.tsv\")\n",
    "hasoc_en_xlsx_2020 = os.path.join(HASOC_FOLDER, \"english_2020.xlsx\")\n",
    "hasoc_hi_xlsx_2020 = os.path.join(HASOC_FOLDER, \"hindi_2020.xlsx\")\n",
    "\n",
    "RESUME_FROM_CHECKPOINT = None\n",
    "\n",
    "print(f\"--- 2. Loading Agnostic Neurons from {analysis_file} ---\")\n",
    "try:\n",
    "    with open(analysis_file, 'r') as f:\n",
    "        agnostic_results = json.load(f)[\"language_agnostic_neurons\"]\n",
    "    agnostic_indices_by_layer = {}\n",
    "    for layer_name, data in agnostic_results.items():\n",
    "        indices = data[\"agnostic_neuron_indices\"]\n",
    "        agnostic_indices_by_layer[layer_name] = torch.tensor(indices, dtype=torch.long)\n",
    "    print(f\"Loaded agnostic neuron indices for {len(agnostic_indices_by_layer)} layers.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Could not load or parse {analysis_file}. Stopping. {e}\")\n",
    "    exit()\n",
    "\n",
    "print(f\"--- 3. Loading original model in bfloat16: {model_id} ---\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=my_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, dtype=torch.bfloat16, device_map=\"auto\", token=my_token\n",
    ")\n",
    "if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "print(\"Original model loaded.\")\n",
    "\n",
    "print(f\"--- 4. Loading and combining BENIGN-ONLY data (EN, HI, MR) ---\")\n",
    "\n",
    "all_texts = [] \n",
    "total_samples_loaded = 0\n",
    "\n",
    "def load_and_extend(filepath, filetype, text_col, sep=','):\n",
    "    try:\n",
    "        if filetype == 'csv':\n",
    "            df = pd.read_csv(filepath)\n",
    "        elif filetype == 'tsv':\n",
    "            df = pd.read_csv(filepath, sep=sep, on_bad_lines='skip')\n",
    "        elif filetype == 'xlsx':\n",
    "            df = pd.read_excel(filepath)\n",
    "        if 'task_1' in df.columns:\n",
    "            label_col = 'task_1'\n",
    "        elif 'task1' in df.columns:\n",
    "            label_col = 'task1'\n",
    "        else:\n",
    "            print(f\"  Warning: No 'task_1' or 'task1' in {filepath}. Skipping file.\")\n",
    "            return 0\n",
    "        benign_df = df[df[label_col] == 'NOT']\n",
    "        texts = benign_df[text_col].dropna().tolist()\n",
    "        count = len(texts)\n",
    "        all_texts.extend(texts)\n",
    "        print(f\"  Loaded {count} BENIGN samples from {filepath}.\")\n",
    "        return count\n",
    "    except Exception as e:\n",
    "        print(f\"  Error loading {filepath}: {e}\")\n",
    "        return 0\n",
    "\n",
    "total_samples_loaded += load_and_extend(hasoc_en_csv_2021, 'csv', 'text')\n",
    "total_samples_loaded += load_and_extend(hasoc_hi_csv_2021, 'csv', 'text')\n",
    "total_samples_loaded += load_and_extend(hasoc_mr_csv_2021, 'csv', 'text')\n",
    "total_samples_loaded += load_and_extend(hasoc_en_tsv_2019_1, 'tsv', 'text', sep='\\t')\n",
    "total_samples_loaded += load_and_extend(hasoc_en_tsv_2019_2, 'tsv', 'text', sep='\\t')\n",
    "total_samples_loaded += load_and_extend(hasoc_hi_tsv_2019_1, 'tsv', 'text', sep='\\t')\n",
    "total_samples_loaded += load_and_extend(hasoc_hi_tsv_2019_2, 'tsv', 'text', sep='\\t')\n",
    "total_samples_loaded += load_and_extend(hasoc_en_xlsx_2020, 'xlsx', 'text')\n",
    "total_samples_loaded += load_and_extend(hasoc_hi_xlsx_2020, 'xlsx', 'text')\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": all_texts})\n",
    "print(f\"\\nTotal combined multilingual BENIGN dataset size: {total_samples_loaded} samples.\")\n",
    "\n",
    "if total_samples_loaded == 0:\n",
    "    print(\"ERROR: No data was loaded. Check your file paths and formats. Stopping.\")\n",
    "    exit()\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=512, padding=\"max_length\")\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "print(\"Dataset tokenized and ready.\")\n",
    "\n",
    "print(\"--- 5. Applying gradient mask to finetune *only* agnostic neurons ---\")\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "gradient_masks = {}\n",
    "for i, layer in enumerate(model.model.layers):\n",
    "    layer_name = f\"model.model.layers.{i}.mlp\"\n",
    "    if layer_name in agnostic_results:\n",
    "        target_weight = layer.mlp.down_proj.weight\n",
    "        target_weight.requires_grad = True \n",
    "        indices_to_train = agnostic_indices_by_layer[layer_name].to(target_weight.device)\n",
    "        mask = torch.zeros_like(target_weight, dtype=torch.bfloat16)\n",
    "        mask[indices_to_train, :] = 1.0\n",
    "        gradient_masks[target_weight] = mask\n",
    "        if i < 5: \n",
    "            print(f\"  Mask applied to {layer_name}. {len(indices_to_train)} neurons will be trained.\")\n",
    "\n",
    "def apply_gradient_mask(model):\n",
    "    for param, mask in gradient_masks.items():\n",
    "        if param.grad is not None:\n",
    "            param.grad.data.mul_(mask)\n",
    "model.register_full_backward_hook(lambda model, grad_in, grad_out: apply_gradient_mask(model))\n",
    "\n",
    "print(\"--- 6. Configuring Trainer ---\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=new_model_name,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    save_total_limit=3,\n",
    "    bf16=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(f\"--- 7. Starting Finetuning on BENIGN-ONLY data ---\")\n",
    "print(f\"Checkpoints will be saved to '{new_model_name}'\")\n",
    "\n",
    "trainer.train(resume_from_checkpoint=RESUME_FROM_CHECKPOINT)\n",
    "\n",
    "print(\"--- 8. Training complete. Saving final model. ---\")\n",
    "trainer.save_model(new_model_name)\n",
    "tokenizer.save_pretrained(new_model_name)\n",
    "\n",
    "print(f\"\\n--- All Done ---\")\n",
    "print(f\"Your new, benign-finetuned model is saved in '{new_model_name}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Loading Configuration ---\n",
      "--- 2. Initializing Perspective API ---\n",
      "  Perspective API client built successfully.\n",
      "--- 3. Loading and randomizing toxic prompts from RTPLX ---\n",
      "Loaded 999 English prompts, randomly sampled 100.\n",
      "Loaded 1115 Hindi prompts, randomly sampled 100.\n",
      "\n",
      "--- 4. Running Generation & Scoring ---\n",
      "\n",
      "--- Processing ORIGINAL Model (meta-llama/Meta-Llama-3.1-8B) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7e29e1d9653469fbb540e9710f6cb34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EN Prompts (Baseline):   0%|          | 0/100 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):   1%|          | 1/100 [00:02<03:57,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):   2%|         | 2/100 [00:04<04:03,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):   3%|         | 3/100 [00:07<04:03,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):   4%|         | 4/100 [00:09<04:01,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):   5%|         | 5/100 [00:12<03:58,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):   6%|         | 6/100 [00:14<03:53,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):   7%|         | 7/100 [00:17<03:54,  2.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):   8%|         | 8/100 [00:19<03:48,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):   9%|         | 9/100 [00:22<03:46,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  10%|         | 10/100 [00:24<03:41,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  11%|         | 11/100 [00:26<03:25,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  12%|        | 12/100 [00:29<03:29,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  13%|        | 13/100 [00:31<03:27,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  14%|        | 14/100 [00:34<03:28,  2.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  15%|        | 15/100 [00:36<03:29,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  16%|        | 16/100 [00:39<03:24,  2.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  17%|        | 17/100 [00:41<03:18,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  18%|        | 18/100 [00:44<03:19,  2.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  19%|        | 19/100 [00:46<03:17,  2.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  20%|        | 20/100 [00:48<03:14,  2.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  21%|        | 21/100 [00:51<03:08,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  22%|       | 22/100 [00:53<03:08,  2.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  23%|       | 23/100 [00:56<03:08,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  24%|       | 24/100 [00:58<03:07,  2.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  25%|       | 25/100 [01:01<03:03,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  26%|       | 26/100 [01:03<02:58,  2.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  27%|       | 27/100 [01:05<02:58,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  28%|       | 28/100 [01:08<02:55,  2.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  29%|       | 29/100 [01:10<02:51,  2.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  30%|       | 30/100 [01:13<02:46,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  31%|       | 31/100 [01:15<02:48,  2.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  32%|      | 32/100 [01:17<02:43,  2.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  33%|      | 33/100 [01:20<02:43,  2.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  34%|      | 34/100 [01:22<02:39,  2.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  35%|      | 35/100 [01:25<02:35,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  36%|      | 36/100 [01:27<02:32,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  37%|      | 37/100 [01:30<02:33,  2.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  38%|      | 38/100 [01:32<02:33,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  39%|      | 39/100 [01:35<02:30,  2.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  40%|      | 40/100 [01:37<02:25,  2.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  41%|      | 41/100 [01:39<02:22,  2.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  42%|     | 42/100 [01:42<02:21,  2.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  43%|     | 43/100 [01:44<02:17,  2.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  44%|     | 44/100 [01:47<02:14,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  45%|     | 45/100 [01:49<02:11,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  46%|     | 46/100 [01:51<02:08,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  47%|     | 47/100 [01:54<02:05,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  48%|     | 48/100 [01:56<02:03,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  49%|     | 49/100 [01:59<02:03,  2.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  50%|     | 50/100 [02:01<02:00,  2.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  51%|     | 51/100 [02:03<01:57,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  52%|    | 52/100 [02:06<01:54,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  53%|    | 53/100 [02:08<01:51,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  54%|    | 54/100 [02:10<01:49,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  55%|    | 55/100 [02:13<01:46,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  56%|    | 56/100 [02:15<01:44,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  57%|    | 57/100 [02:17<01:42,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  58%|    | 58/100 [02:20<01:40,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  59%|    | 59/100 [02:22<01:40,  2.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  60%|    | 60/100 [02:25<01:36,  2.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  61%|    | 61/100 [02:27<01:33,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  62%|   | 62/100 [02:30<01:30,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  63%|   | 63/100 [02:32<01:28,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  64%|   | 64/100 [02:34<01:25,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  65%|   | 65/100 [02:37<01:23,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  66%|   | 66/100 [02:39<01:20,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  67%|   | 67/100 [02:41<01:18,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  68%|   | 68/100 [02:44<01:16,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  69%|   | 69/100 [02:46<01:15,  2.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  70%|   | 70/100 [02:49<01:11,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  71%|   | 71/100 [02:51<01:10,  2.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  72%|  | 72/100 [02:53<01:07,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  73%|  | 73/100 [02:56<01:04,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  74%|  | 74/100 [02:58<01:01,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  75%|  | 75/100 [03:01<00:59,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  76%|  | 76/100 [03:03<00:54,  2.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  77%|  | 77/100 [03:05<00:52,  2.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  78%|  | 78/100 [03:07<00:50,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  79%|  | 79/100 [03:10<00:48,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  80%|  | 80/100 [03:12<00:46,  2.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  81%|  | 81/100 [03:14<00:44,  2.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  82%| | 82/100 [03:17<00:41,  2.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  83%| | 83/100 [03:19<00:39,  2.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  84%| | 84/100 [03:21<00:37,  2.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  85%| | 85/100 [03:24<00:35,  2.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  86%| | 86/100 [03:26<00:32,  2.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  87%| | 87/100 [03:28<00:30,  2.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  88%| | 88/100 [03:31<00:28,  2.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  89%| | 89/100 [03:33<00:25,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  90%| | 90/100 [03:35<00:23,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  91%| | 91/100 [03:38<00:21,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  92%|| 92/100 [03:40<00:18,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  93%|| 93/100 [03:43<00:16,  2.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  94%|| 94/100 [03:45<00:14,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  95%|| 95/100 [03:47<00:11,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  96%|| 96/100 [03:50<00:09,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  97%|| 97/100 [03:52<00:07,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  98%|| 98/100 [03:54<00:04,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline):  99%|| 99/100 [03:57<00:02,  2.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Baseline): 100%|| 100/100 [03:59<00:00,  2.40s/it]\n",
      "HI Prompts (Baseline):   0%|          | 0/100 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):   1%|          | 1/100 [00:02<04:00,  2.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):   2%|         | 2/100 [00:04<03:53,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):   3%|         | 3/100 [00:07<03:51,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):   4%|         | 4/100 [00:09<03:48,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):   5%|         | 5/100 [00:11<03:48,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):   6%|         | 6/100 [00:14<03:45,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):   7%|         | 7/100 [00:16<03:43,  2.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):   8%|         | 8/100 [00:19<03:44,  2.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):   9%|         | 9/100 [00:21<03:39,  2.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  10%|         | 10/100 [00:24<03:35,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  11%|         | 11/100 [00:26<03:32,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  12%|        | 12/100 [00:28<03:28,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  13%|        | 13/100 [00:31<03:27,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  14%|        | 14/100 [00:33<03:25,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  15%|        | 15/100 [00:35<03:21,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  16%|        | 16/100 [00:38<03:19,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  17%|        | 17/100 [00:40<03:17,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  18%|        | 18/100 [00:43<03:14,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  19%|        | 19/100 [00:45<03:13,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  20%|        | 20/100 [00:47<03:11,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  21%|        | 21/100 [00:50<03:07,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  22%|       | 22/100 [00:52<03:05,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  23%|       | 23/100 [00:54<03:03,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  24%|       | 24/100 [00:57<03:00,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  25%|       | 25/100 [00:59<02:58,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  26%|       | 26/100 [01:02<02:56,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  27%|       | 27/100 [01:04<02:54,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  28%|       | 28/100 [01:06<02:52,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  29%|       | 29/100 [01:09<02:49,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  30%|       | 30/100 [01:11<02:47,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  31%|       | 31/100 [01:14<02:44,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  32%|      | 32/100 [01:16<02:42,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  33%|      | 33/100 [01:18<02:38,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  34%|      | 34/100 [01:21<02:36,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  35%|      | 35/100 [01:23<02:34,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  36%|      | 36/100 [01:25<02:31,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  37%|      | 37/100 [01:28<02:29,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  38%|      | 38/100 [01:30<02:26,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  39%|      | 39/100 [01:32<02:23,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  40%|      | 40/100 [01:35<02:20,  2.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  41%|      | 41/100 [01:37<02:20,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  42%|     | 42/100 [01:40<02:18,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  43%|     | 43/100 [01:42<02:17,  2.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  44%|     | 44/100 [01:44<02:14,  2.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  45%|     | 45/100 [01:47<02:11,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  46%|     | 46/100 [01:49<02:08,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  47%|     | 47/100 [01:52<02:05,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  48%|     | 48/100 [01:54<02:02,  2.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  49%|     | 49/100 [01:56<01:59,  2.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  50%|     | 50/100 [01:59<01:58,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  51%|     | 51/100 [02:01<01:55,  2.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  52%|    | 52/100 [02:03<01:52,  2.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  53%|    | 53/100 [02:06<01:50,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  54%|    | 54/100 [02:08<01:47,  2.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  55%|    | 55/100 [02:10<01:45,  2.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  56%|    | 56/100 [02:13<01:43,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  57%|    | 57/100 [02:15<01:42,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  58%|    | 58/100 [02:17<01:38,  2.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  59%|    | 59/100 [02:20<01:36,  2.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  60%|    | 60/100 [02:22<01:33,  2.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  61%|    | 61/100 [02:24<01:31,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  62%|   | 62/100 [02:27<01:29,  2.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  63%|   | 63/100 [02:29<01:26,  2.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  64%|   | 64/100 [02:32<01:25,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  65%|   | 65/100 [02:34<01:23,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  66%|   | 66/100 [02:36<01:21,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  67%|   | 67/100 [02:39<01:18,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  68%|   | 68/100 [02:41<01:15,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  69%|   | 69/100 [02:43<01:13,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  70%|   | 70/100 [02:46<01:10,  2.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  71%|   | 71/100 [02:48<01:08,  2.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  72%|  | 72/100 [02:50<01:05,  2.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  73%|  | 73/100 [02:53<01:03,  2.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  74%|  | 74/100 [02:55<01:01,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  75%|  | 75/100 [02:57<00:58,  2.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  76%|  | 76/100 [03:00<00:55,  2.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  77%|  | 77/100 [03:02<00:53,  2.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  78%|  | 78/100 [03:04<00:51,  2.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  79%|  | 79/100 [03:07<00:48,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  80%|  | 80/100 [03:09<00:46,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  81%|  | 81/100 [03:11<00:43,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  82%| | 82/100 [03:14<00:41,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  83%| | 83/100 [03:16<00:39,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  84%| | 84/100 [03:18<00:37,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  85%| | 85/100 [03:21<00:34,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  86%| | 86/100 [03:23<00:32,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  87%| | 87/100 [03:25<00:30,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  88%| | 88/100 [03:27<00:27,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  89%| | 89/100 [03:30<00:25,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  90%| | 90/100 [03:32<00:22,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  91%| | 91/100 [03:34<00:20,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  92%|| 92/100 [03:37<00:18,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  93%|| 93/100 [03:39<00:16,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  94%|| 94/100 [03:41<00:13,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  95%|| 95/100 [03:44<00:11,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  96%|| 96/100 [03:46<00:09,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  97%|| 97/100 [03:48<00:06,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  98%|| 98/100 [03:51<00:04,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline):  99%|| 99/100 [03:53<00:02,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Baseline): 100%|| 100/100 [03:55<00:00,  2.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Clearing VRAM before loading next model ---\n",
      "\n",
      "--- Processing FINETUNED Model (./llama-3.1-8b-agnostic-finetuned-BENIGN-ONLY) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "346ab1a19d794e5faaf671d9f339eb89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EN Prompts (Finetuned):   0%|          | 0/100 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):   1%|          | 1/100 [00:02<03:49,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):   2%|         | 2/100 [00:04<03:46,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):   3%|         | 3/100 [00:06<03:44,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):   4%|         | 4/100 [00:09<03:40,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):   5%|         | 5/100 [00:11<03:38,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):   6%|         | 6/100 [00:13<03:35,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):   7%|         | 7/100 [00:16<03:33,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):   8%|         | 8/100 [00:18<03:30,  2.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):   9%|         | 9/100 [00:20<03:28,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  10%|         | 10/100 [00:22<03:26,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  11%|         | 11/100 [00:25<03:24,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  12%|        | 12/100 [00:27<03:22,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  13%|        | 13/100 [00:29<03:20,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  14%|        | 14/100 [00:32<03:18,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  15%|        | 15/100 [00:34<03:14,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  16%|        | 16/100 [00:36<03:13,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  17%|        | 17/100 [00:39<03:11,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  18%|        | 18/100 [00:41<03:09,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  19%|        | 19/100 [00:43<03:06,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  20%|        | 20/100 [00:46<03:04,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  21%|        | 21/100 [00:47<02:50,  2.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  22%|       | 22/100 [00:50<02:50,  2.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  23%|       | 23/100 [00:52<02:52,  2.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  24%|       | 24/100 [00:54<02:51,  2.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  25%|       | 25/100 [00:57<02:49,  2.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  26%|       | 26/100 [00:59<02:48,  2.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  27%|       | 27/100 [01:01<02:47,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  28%|       | 28/100 [01:03<02:44,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  29%|       | 29/100 [01:06<02:42,  2.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  30%|       | 30/100 [01:08<02:38,  2.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  31%|       | 31/100 [01:10<02:36,  2.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  32%|      | 32/100 [01:12<02:33,  2.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  33%|      | 33/100 [01:15<02:32,  2.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  34%|      | 34/100 [01:17<02:30,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  35%|      | 35/100 [01:19<02:27,  2.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  36%|      | 36/100 [01:22<02:25,  2.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  37%|      | 37/100 [01:24<02:23,  2.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  38%|      | 38/100 [01:26<02:20,  2.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  39%|      | 39/100 [01:28<02:18,  2.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  40%|      | 40/100 [01:31<02:16,  2.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  41%|      | 41/100 [01:33<02:14,  2.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  42%|     | 42/100 [01:35<02:12,  2.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  43%|     | 43/100 [01:38<02:09,  2.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  44%|     | 44/100 [01:40<02:07,  2.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  45%|     | 45/100 [01:42<02:05,  2.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  46%|     | 46/100 [01:44<02:03,  2.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  47%|     | 47/100 [01:47<02:01,  2.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  48%|     | 48/100 [01:49<01:58,  2.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  49%|     | 49/100 [01:51<01:56,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  50%|     | 50/100 [01:54<01:54,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  51%|     | 51/100 [01:56<01:52,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  52%|    | 52/100 [01:58<01:50,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  53%|    | 53/100 [02:00<01:47,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  54%|    | 54/100 [02:03<01:45,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  55%|    | 55/100 [02:05<01:43,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  56%|    | 56/100 [02:07<01:41,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  57%|    | 57/100 [02:10<01:39,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  58%|    | 58/100 [02:12<01:37,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  59%|    | 59/100 [02:14<01:34,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  60%|    | 60/100 [02:17<01:32,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  61%|    | 61/100 [02:19<01:29,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  62%|   | 62/100 [02:21<01:27,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  63%|   | 63/100 [02:24<01:25,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  64%|   | 64/100 [02:26<01:22,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  65%|   | 65/100 [02:28<01:20,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  66%|   | 66/100 [02:30<01:17,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  67%|   | 67/100 [02:33<01:15,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  68%|   | 68/100 [02:35<01:13,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  69%|   | 69/100 [02:37<01:11,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  70%|   | 70/100 [02:40<01:08,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  71%|   | 71/100 [02:42<01:06,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  72%|  | 72/100 [02:44<01:04,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  73%|  | 73/100 [02:47<01:02,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  74%|  | 74/100 [02:49<00:59,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  75%|  | 75/100 [02:51<00:57,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  76%|  | 76/100 [02:53<00:55,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  77%|  | 77/100 [02:56<00:52,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  78%|  | 78/100 [02:58<00:50,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  79%|  | 79/100 [03:00<00:48,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  80%|  | 80/100 [03:03<00:45,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  81%|  | 81/100 [03:05<00:43,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  82%| | 82/100 [03:07<00:41,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  83%| | 83/100 [03:09<00:39,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  84%| | 84/100 [03:12<00:36,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  85%| | 85/100 [03:14<00:34,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  86%| | 86/100 [03:16<00:32,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  87%| | 87/100 [03:19<00:29,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  88%| | 88/100 [03:21<00:27,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  89%| | 89/100 [03:23<00:25,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  90%| | 90/100 [03:26<00:23,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  91%| | 91/100 [03:28<00:20,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  92%|| 92/100 [03:30<00:18,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  93%|| 93/100 [03:32<00:14,  2.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  94%|| 94/100 [03:34<00:13,  2.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  95%|| 95/100 [03:37<00:11,  2.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  96%|| 96/100 [03:39<00:09,  2.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  97%|| 97/100 [03:41<00:06,  2.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  98%|| 98/100 [03:43<00:04,  2.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned):  99%|| 99/100 [03:46<00:02,  2.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "EN Prompts (Finetuned): 100%|| 100/100 [03:48<00:00,  2.29s/it]\n",
      "HI Prompts (Finetuned):   0%|          | 0/100 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):   1%|          | 1/100 [00:02<03:47,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):   2%|         | 2/100 [00:04<03:43,  2.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):   3%|         | 3/100 [00:06<03:41,  2.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):   4%|         | 4/100 [00:09<03:39,  2.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):   5%|         | 5/100 [00:11<03:37,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):   6%|         | 6/100 [00:13<03:34,  2.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):   7%|         | 7/100 [00:15<03:32,  2.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):   8%|         | 8/100 [00:18<03:30,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):   9%|         | 9/100 [00:20<03:28,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  10%|         | 10/100 [00:22<03:25,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  11%|         | 11/100 [00:25<03:23,  2.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  12%|        | 12/100 [00:27<03:26,  2.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  13%|        | 13/100 [00:29<03:23,  2.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  14%|        | 14/100 [00:32<03:21,  2.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  15%|        | 15/100 [00:34<03:17,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  16%|        | 16/100 [00:37<03:18,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  17%|        | 17/100 [00:39<03:13,  2.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  18%|        | 18/100 [00:41<03:10,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  19%|        | 19/100 [00:43<03:07,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  20%|        | 20/100 [00:46<03:04,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  21%|        | 21/100 [00:48<03:04,  2.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  22%|       | 22/100 [00:50<03:01,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  23%|       | 23/100 [00:53<02:58,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  24%|       | 24/100 [00:55<02:55,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  25%|       | 25/100 [00:57<02:57,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  26%|       | 26/100 [01:00<02:52,  2.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  27%|       | 27/100 [01:02<02:50,  2.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  28%|       | 28/100 [01:05<02:51,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  29%|       | 29/100 [01:07<02:48,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  30%|       | 30/100 [01:09<02:46,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  31%|       | 31/100 [01:12<02:42,  2.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  32%|      | 32/100 [01:14<02:42,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  33%|      | 33/100 [01:16<02:38,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  34%|      | 34/100 [01:19<02:35,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  35%|      | 35/100 [01:21<02:32,  2.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  36%|      | 36/100 [01:23<02:29,  2.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  37%|      | 37/100 [01:26<02:26,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  38%|      | 38/100 [01:28<02:23,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  39%|      | 39/100 [01:30<02:21,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  40%|      | 40/100 [01:33<02:18,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  41%|      | 41/100 [01:35<02:15,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  42%|     | 42/100 [01:37<02:13,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  43%|     | 43/100 [01:40<02:13,  2.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  44%|     | 44/100 [01:42<02:10,  2.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  45%|     | 45/100 [01:44<02:08,  2.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  46%|     | 46/100 [01:47<02:06,  2.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  47%|     | 47/100 [01:49<02:05,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  48%|     | 48/100 [01:51<02:04,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  49%|     | 49/100 [01:54<02:03,  2.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  50%|     | 50/100 [01:56<01:59,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  51%|     | 51/100 [01:59<01:55,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  52%|    | 52/100 [02:01<01:52,  2.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  53%|    | 53/100 [02:03<01:49,  2.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  54%|    | 54/100 [02:06<01:47,  2.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  55%|    | 55/100 [02:08<01:47,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  56%|    | 56/100 [02:10<01:43,  2.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  57%|    | 57/100 [02:13<01:40,  2.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  58%|    | 58/100 [02:15<01:38,  2.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  59%|    | 59/100 [02:17<01:35,  2.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  60%|    | 60/100 [02:20<01:32,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  61%|    | 61/100 [02:22<01:32,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  62%|   | 62/100 [02:24<01:29,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  63%|   | 63/100 [02:27<01:27,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  64%|   | 64/100 [02:29<01:24,  2.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  65%|   | 65/100 [02:32<01:23,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  66%|   | 66/100 [02:34<01:20,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  67%|   | 67/100 [02:36<01:17,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  68%|   | 68/100 [02:39<01:15,  2.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  69%|   | 69/100 [02:41<01:12,  2.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  70%|   | 70/100 [02:43<01:10,  2.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  71%|   | 71/100 [02:45<01:07,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  72%|  | 72/100 [02:48<01:05,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  73%|  | 73/100 [02:50<01:02,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  74%|  | 74/100 [02:52<01:00,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  75%|  | 75/100 [02:55<00:57,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  76%|  | 76/100 [02:57<00:55,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  77%|  | 77/100 [03:00<00:54,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  78%|  | 78/100 [03:02<00:51,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  79%|  | 79/100 [03:04<00:49,  2.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  80%|  | 80/100 [03:07<00:46,  2.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  81%|  | 81/100 [03:09<00:43,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  82%| | 82/100 [03:11<00:41,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  83%| | 83/100 [03:13<00:38,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  84%| | 84/100 [03:16<00:36,  2.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  85%| | 85/100 [03:18<00:34,  2.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  86%| | 86/100 [03:20<00:32,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  87%| | 87/100 [03:23<00:29,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  88%| | 88/100 [03:25<00:27,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  89%| | 89/100 [03:27<00:25,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  90%| | 90/100 [03:29<00:22,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  91%| | 91/100 [03:32<00:20,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  92%|| 92/100 [03:34<00:18,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  93%|| 93/100 [03:36<00:16,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  94%|| 94/100 [03:39<00:13,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  95%|| 95/100 [03:41<00:11,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  96%|| 96/100 [03:43<00:09,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  97%|| 97/100 [03:46<00:07,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  98%|| 98/100 [03:48<00:04,  2.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned):  99%|| 99/100 [03:50<00:02,  2.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "HI Prompts (Finetuned): 100%|| 100/100 [03:53<00:00,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "--- FINAL RESULTS (Average Toxicity) ---\n",
      "\n",
      "--- ENGLISH Test (100 Prompts) ---\n",
      "  Average Baseline Toxicity:  0.3968\n",
      "  Average Finetuned Toxicity: 0.6310\n",
      "    (Reduction: -0.2342)\n",
      "\n",
      "--- HINDI Test (100 Prompts) ---\n",
      "  Average Baseline Toxicity:  0.3651\n",
      "  Average Finetuned Toxicity: 0.2832\n",
      "    (Reduction: 0.0819)\n",
      "\n",
      "--- MIXED (Average) Test (200 Prompts) ---\n",
      "  Average Baseline Toxicity:  0.3809\n",
      "  Average Finetuned Toxicity: 0.4571\n",
      "    (Reduction: -0.0762)\n",
      "\n",
      "--- Validation complete. ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"--- 1. Loading Configuration ---\")\n",
    "\n",
    "model_id_original = \"meta-llama/Meta-Llama-3.1-8B\"\n",
    "model_id_finetuned = \"./llama-3.1-8b-agnostic-finetuned-BENIGN-ONLY\"\n",
    "\n",
    "rtplx_folder = \"rtplx\"\n",
    "rtplx_en_file = \"RTP_LX_EN.json\"\n",
    "rtplx_hi_file = \"RTP_LX_HI.json\"\n",
    "\n",
    "NUM_PROMPTS_TO_TEST = 100\n",
    "\n",
    "print(\"--- 2. Initializing Perspective API ---\")\n",
    "try:\n",
    "    perspective = build(\n",
    "        'commentanalyzer',\n",
    "        'v1alpha1',\n",
    "        developerKey=API_KEY,\n",
    "        discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
    "        static_discovery=False\n",
    "    )\n",
    "    print(\"  Perspective API client built successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Could not initialize Perspective API: {e}\")\n",
    "    perspective = None\n",
    "\n",
    "def get_toxicity_score(text, lang='en'):\n",
    "    if perspective is None: return None\n",
    "    if not text.strip() or \"bool]bool\" in text or \"otasotas\" in text:\n",
    "        return 0.0\n",
    "    time.sleep(1.2)\n",
    "    analyze_request = {\n",
    "        'comment': {'text': text},\n",
    "        'requestedAttributes': {'TOXICITY': {}},\n",
    "        'languages': [lang]\n",
    "    }\n",
    "    try:\n",
    "        response = perspective.comments().analyze(body=analyze_request).execute()\n",
    "        score = response['attributeScores']['TOXICITY']['summaryScore']['value']\n",
    "        return round(score, 4)\n",
    "    except HttpError as e:\n",
    "        if e.resp.status == 429:\n",
    "            print(\"  [Perspective API Error: Quota exceeded. Sleeping for 5s...]\")\n",
    "            time.sleep(5)\n",
    "            return get_toxicity_score(text, lang)\n",
    "        print(f\"  [Perspective API Error for lang={lang}: {e}]\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"  [Unexpected API Error for lang={lang}: {e}]\")\n",
    "        return None\n",
    "\n",
    "print(f\"--- 3. Loading and randomizing toxic prompts from RTPLX ---\")\n",
    "toxic_prompts_en, toxic_prompts_hi = [], []\n",
    "try:\n",
    "    filepath_en = os.path.join(rtplx_folder, rtplx_en_file)\n",
    "    all_en_prompts = [json.loads(line)[\"Prompt\"] for line in open(filepath_en, 'r', encoding='utf-8')]\n",
    "    num_to_sample_en = min(NUM_PROMPTS_TO_TEST, len(all_en_prompts))\n",
    "    toxic_prompts_en = random.sample(all_en_prompts, num_to_sample_en)\n",
    "    print(f\"Loaded {len(all_en_prompts)} English prompts, randomly sampled {len(toxic_prompts_en)}.\")\n",
    "    del all_en_prompts\n",
    "\n",
    "    filepath_hi = os.path.join(rtplx_folder, rtplx_hi_file)\n",
    "    all_hi_prompts = [json.loads(line)[\"Prompt\"] for line in open(filepath_hi, 'r', encoding='utf-8')]\n",
    "    num_to_sample_hi = min(NUM_PROMPTS_TO_TEST, len(all_hi_prompts))\n",
    "    toxic_prompts_hi = random.sample(all_hi_prompts, num_to_sample_hi)\n",
    "    print(f\"Loaded {len(all_hi_prompts)} Hindi prompts, randomly sampled {len(toxic_prompts_hi)}.\")\n",
    "    del all_hi_prompts\n",
    "except Exception as e:\n",
    "    print(f\"Error loading test prompts: {e}\")\n",
    "    exit()\n",
    "\n",
    "print(f\"\\n--- 4. Running Generation & Scoring ---\")\n",
    "generation_kwargs = {\"max_new_tokens\": 60, \"do_sample\": False}\n",
    "all_scores = {\n",
    "    \"baseline_en\": [], \"finetuned_en\": [],\n",
    "    \"baseline_hi\": [], \"finetuned_hi\": []\n",
    "}\n",
    "\n",
    "print(f\"\\n--- Processing ORIGINAL Model ({model_id_original}) ---\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id_original, token=my_token)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id_original, dtype=torch.bfloat16, device_map=\"auto\", token=my_token\n",
    "    )\n",
    "    model.eval()\n",
    "    if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"left\"\n",
    "\n",
    "    for prompt in tqdm(toxic_prompts_en, desc=\"EN Prompts (Baseline)\"):\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "        with torch.no_grad(): output = model.generate(**inputs, **generation_kwargs)\n",
    "        text = tokenizer.decode(output[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "        score = get_toxicity_score(text, lang='en')\n",
    "        if score is not None: all_scores[\"baseline_en\"].append(score)\n",
    "\n",
    "    for prompt in tqdm(toxic_prompts_hi, desc=\"HI Prompts (Baseline)\"):\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "        with torch.no_grad(): output = model.generate(**inputs, **generation_kwargs)\n",
    "        text = tokenizer.decode(output[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "        score = get_toxicity_score(text, lang='hi')\n",
    "        if score is not None: all_scores[\"baseline_hi\"].append(score)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during Baseline model testing: {e}\")\n",
    "\n",
    "print(\"--- Clearing VRAM before loading next model ---\")\n",
    "del model\n",
    "del tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\n--- Processing FINETUNED Model ({model_id_finetuned}) ---\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id_finetuned)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id_finetuned, dtype=torch.bfloat16, device_map=\"auto\"\n",
    "    )\n",
    "    model.eval()\n",
    "    if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"left\"\n",
    "\n",
    "    for prompt in tqdm(toxic_prompts_en, desc=\"EN Prompts (Finetuned)\"):\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "        with torch.no_grad(): output = model.generate(**inputs, **generation_kwargs)\n",
    "        text = tokenizer.decode(output[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "        score = get_toxicity_score(text, lang='en')\n",
    "        if score is not None: all_scores[\"finetuned_en\"].append(score)\n",
    "\n",
    "    for prompt in tqdm(toxic_prompts_hi, desc=\"HI Prompts (Finetuned)\"):\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "        with torch.no_grad(): output = model.generate(**inputs, **generation_kwargs)\n",
    "        text = tokenizer.decode(output[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "        score = get_toxicity_score(text, lang='hi')\n",
    "        if score is not None: all_scores[\"finetuned_hi\"].append(score)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during Finetuned model testing: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"--- FINAL RESULTS (Average Toxicity) ---\")\n",
    "\n",
    "def get_avg_score(key):\n",
    "    if all_scores[key]:\n",
    "        return np.mean(all_scores[key])\n",
    "    return 0.0\n",
    "\n",
    "avg_baseline_en = get_avg_score(\"baseline_en\")\n",
    "avg_finetuned_en = get_avg_score(\"finetuned_en\")\n",
    "avg_baseline_hi = get_avg_score(\"baseline_hi\")\n",
    "avg_finetuned_hi = get_avg_score(\"finetuned_hi\")\n",
    "\n",
    "avg_baseline_mix = np.mean([avg_baseline_en, avg_baseline_hi]) if all_scores[\"baseline_en\"] and all_scores[\"baseline_hi\"] else 0\n",
    "avg_finetuned_mix = np.mean([avg_finetuned_en, avg_finetuned_hi]) if all_scores[\"finetuned_en\"] and all_scores[\"finetuned_hi\"] else 0\n",
    "\n",
    "def print_summary(name, baseline, finetuned, count):\n",
    "    print(f\"\\n--- {name} Test ({count} Prompts) ---\")\n",
    "    reduction = baseline - finetuned\n",
    "    print(f\"  Average Baseline Toxicity:  {baseline:.4f}\")\n",
    "    print(f\"  Average Finetuned Toxicity: {finetuned:.4f}\")\n",
    "    print(f\"    (Reduction: {reduction:.4f})\")\n",
    "\n",
    "print_summary(\"ENGLISH\", avg_baseline_en, avg_finetuned_en, len(all_scores[\"baseline_en\"]))\n",
    "print_summary(\"HINDI\", avg_baseline_hi, avg_finetuned_hi, len(all_scores[\"baseline_hi\"]))\n",
    "print_summary(\"MIXED (Average)\", avg_baseline_mix, avg_finetuned_mix, len(all_scores[\"baseline_en\"]) + len(all_scores[\"baseline_hi\"]))\n",
    "\n",
    "print(\"\\n--- Validation complete. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
